\chapter{\iflanguage{english}{Methods}{Methoden}}
\label{cha:methods}

Im Folgenden Kapitel wird auf die in dieser Arbeit genutzte Hardware und SDKs eingegangen, sowie auf die verschiedenen Ansätze zum Versuchsaufbau, die verfolgt wurden. Es wird des Kalibrierungsvorgang erläutert und zuletzt das User Interface, welches genutzt wurde.
%TODO mehr

\section{Microsoft Hololens}

Das in dieser Arbeit genutzte Optical See-Through HMD ist die Hololens der ersten Generation. Die Hololens wurde 2016 von der Microsoft Corporation auf den Markt gebracht. Sie übertrifft andere kommerziell verfügbaren OST-HMDs in Kontrast und Stabilität der Bildwiederholrate und hat eine geringere Zeitverzögerung \cite{qian_comparison_2017}. Die Hololens ist nicht kabelgebunden, lässt sich über Handgesten sowie Sprachbefehle steuern und gefährdet somit nicht die Sterilität im Operationssaal \cite{pratt_through_2018}.

Ein virtuelles Objekt, welches von der Hololens angezeigt wird, wird \emph{Hologramm} genannt. Die Hololens projiziert ein Hologramm auf zwei Durchsichtdisplays (\emph{Waveguides)}, für jedes Auge eines. Im Gegensatz zu Video See-Through HMDs, welche bei einem Stromausfall dem Nutzer jegliche Sicht versperren, ermöglichen die Waveguides zeitgleich die Sicht auf die reale Umgebung. Für jeden Waveguide gibt es eine \emph{Light Engine}, welche die holographischen Inhalte projiziert. Mit Hilfe von vier Umgebungssensoren und einer Time-of-Flight Tiefenkamera kann die Hololens eine Karte des Raumes erstellen (\emph{Spatial Mapping}) und sich selbst darin Positionieren. Die Tiefenkamera wird zusätzlich zur Erkennung der Handgesten genutzt. \cite{varnauld_mixed_nodate}
%TODO richtiges Zitat Mixed Reality Documentation

\textbf{Technische Daten}

\begin{center}
\begin{tabular}{ll}
\hline
\textbf{Komponente}    & \textbf{Daten}   \\
\hline
\textbf{Optik}      & 2 Durchsichtdisplays (Waveguides)\\
          & 2 HD 16:9 Light Engines \\
          &\\
\textbf{Holographische Auflösung} & 2,3 Millionen Lichtpunkte\\
  &\\
\textbf{Holographische Dichte} & mehr als 2500 Radianten \\
&(2500 Lichtpunkte pro Radiant, \\
& wobei $1 rad \approx 57,3^\circ $ ) \\
  &\\
\textbf{Sensoren}       & 1 IMU (Inertiale Messeinheit)       \\
& 4 Umgebungssensoren \\
& 1 Tiefenkamera (Time-of-Flight) \\
&1 2MP HD Kamera \\
&4 Mikrofone\\
&1 Umgebungslichtsensor\\
  &\\
\textbf{Interaktion}       & räumlicher Ton           \\
& Blickverfolgung\\
& Handgesten \\
& Sprachsteuerung (bei Internetverbingung)\\
  &\\
\textbf{Leistung} & 2-3 Stunden aktive Nutzung            \\
  &bis zu 2 Wochen standby\\
  &nutzbar während des Ladevorgangs\\
  &passive Kühlung\\
  &\\
\textbf{Prozessoren} &    Intel 32 bit architecture \\
& Microsoft Holographic Processing Unit (HPU 1.0)         \\
  &\\
\textbf{Betriebssystem} & Windows 10           \\
\hline
\end{tabular}
\end{center}



\section{NDI Polaris Vega}

Die Polaris Vega von Northern Digital Inc. ist ein Infrarot Tracking System zur Anwendung im medizinischen Bereich. Die Polaris ist als outside-looking-in System konfiguriert und somit nicht sehr mobil. Die zu trackenden Objekte werden mit Markern ausgestattet und von einem stationären Infrarotsensor getrackt. Die Polaris kann sowohl aktive als auch passive Marker tracken. Aktive Marker werden durch ein elektrisches Signal aktiviert und senden Infrarotstrahlung aus. Passive Marker besitzen mindestens drei reflektive Kugeln. Der Infrarotsensor der Polaris sendet Infrarotstrahlung aus, welche von den passiven Markern reflektiert wird. Anhand der erhaltenen Informationen kann der Infrarotsensor die Position und Rotation der Marker bestimmen. 

\textbf{Technische Daten}

%https://www.ndigital.com/medical/products/polaris-family/ 11.08.2019
%https://www.ndigital.com/medical/products/polaris-vega/ 20.08.2019

\begin{center}
\begin{tabular}{ll}
\hline
\textbf{Komponente}    & \textbf{Daten}   \\
\hline
\textbf{Marker}      & passiv, kabellos aktiv\\
          &\\
\textbf{Max. Anzahl passiver Marker} & 25 passive Marker\\
  &\\
\textbf{Max. Update Rate} & 60Hz (standard) / 250Hz (optional) \\
&\\
\textbf{Infrarotsensor Maße} & 591 mm x 103 mm x 106 mm\\
&\\
\textbf{Infrarotsensor Anbringung} &  Anbringung erhöht an Wand oder Stativ\\
&Fixierung an Rückseite\\
&\\
\textbf{Max. Messvolumen} & Beginn Messung in 950 mm Entfernung von Infrarotsensor\\ & 1856 mm x 1470 mm x 2050 mm (Breite x Höhe x Tiefe)\\
\hline
\end{tabular}
\end{center}

\section{Unity}

\emph{Unity} ist eine Gaming Engine entwickelt von Unity Technologies. Sie kann zum Entwickeln von 3D- und 2D-Spielen genutzt werden, sowie unter anderem zum Erstellen von AR- und VR-Anwendungen. Unity unterstützt mehr als 25 Plattformen, darunter auch die Universal Windows Plattform, welche auf der Hololens genutzt wird. Microsoft empfiehlt Unity zum Entwickeln von MR-Anwendungen für die Hololens, mit der Begründung, dass Unity 'die schnellste Methode zum Erstellen einer Mixed Reality-App' sei \cite{varnauld_mixed_nodate}.
%Wie die MR Docs zitieren?
In dieser Arbeit wurde Unity 2018.2.21f1 genutzt.

\section{Mixed Reality Toolkit}

Das \emph{Mixed Reality Toolkit} (MRTK) für Unity ist ein von Microsoft geleitetes Open Source Projekt zur Unterstützung und Beschleunigung der Entwicklung für die Hololens, Windows Mixed Reality immersive (VR) Headsets und OpenVR. Es enthält verschiedene Bauteile in Form von Skripten und vorgefertigten Objekten (Assets). 
Eines dieser Assets ist das \emph{Input Manager Prefab}, das sich unter anderem aus einem \emph{Gaze Manager}, einem \emph{Input Manager} und einem \emph{Focus Manager} Skript zusammensetzt. Mit Hilfe dieser Skripte lässt sich der Blick des Nutzers verfolgen und Gesten erkennen. Die Verfolgung der Blickrichtung ermöglicht das Anzeigen eines Cursors für den Nutzer und dadurch das Anfokussieren einzelner Objekte in der Szene. So besteht die Möglichkeit, Objekte bei Fokussierung farblich hervorzuheben oder weitere Informationen anzuzeigen, die sonst nicht sichtbar sind. 
In dieser Arbeit wurde die MRTK Version 2017.4.3.0 verwendet.
%https://microsoft.github.io/MixedRealityToolkit-Unity/README.html 11.08.2019

\section{Vuforia Augmented Reality SDK}

Vuforia ist ein plattformübergreifendes Software Development Kit zur Entwicklung von Mixed Reality Anwendungen. Vuforia lässt sich sowohl mit iOS- und Android-Geräten als auch mit der Hololens nutzen. Es ist in Unity integriert und ermöglicht durch Methoden der Computer Vision das Tracken von Markern in verschiedenen Formen, wie z.B. das Tracken eines einzelnen Bildes, 3D-Konfigurationen mit mehreren Bildern oder eines 3D Objektes. Vuforia ist nicht open source. Die in dieser Arbeit genutzte Version der Vuforia Engine ist 8.0.10.

Vuforia stellt in der Unity Szene ein eigenes Kamera-Asset zur Verfügung, welches in dieser Arbeit Anstelle des MixedRealityCameraParent-Assets aus dem Mixed Reality Toolkit verwendet wird.

\subsection{Vuforia Model Targets}

%https://library.vuforia.com/features/objects/model-targets.html 12.08.2019
\emph{Vuforia Model Targets} ermöglichen es, Objekte aus der realen Welt basierend auf ihrer Form zu Tracken. Dabei kann es sich um ein kleines Objekt wie ein Spielzeugauto handeln, aber auch um ein großes Objekt, wie ein richtiges Auto. Eine Voraussetzung hierfür ist jedoch, dass ein 3D-Modell (bspw. ein CAD-Modell) des Objektes existiert. Damit das Tracking eines Model Targets gut funktioniert, müssen sowohl das Objekt, als auch das CAD-Modell einige Anforderungen erfüllen.   

Ein Objekt, das getrackt werden soll, sollte:
\begin{itemize}
%Fixed Position in Space
\item Fix im Raum angebracht sein

%Colored or Patterned surface
\item Eine farbige oder gemusterte Oberfläche besitzen

%Sufficient Geometric Detail
\item Komplex sein

%non-flexible and rigid
\item Keine beweglichen Teile haben und nicht verformbar sein
\end{itemize}
%TODO genauer?


Das CAD-Modell sollte:
\begin{itemize}
\item Keine Löcher oder Risse aufweisen
\item Keine fehlenden Teile haben
\item Keine Normalen aufweisen, die in eine andere Richtung als die Oberflächennormale zeigen
\item Keine fehlenden Farbinformationen oder fehlende Textur aufweisen
\item Keine falsche Textur aufweisen
\end{itemize}


\textbf{Vuforia Model Target Generator}

Um ein Model Target für die Verwendung in Unity zu erstellen, hat Vuforia den \emph{Model Target Generator} (MTG) entwickelt. Mit Hilfe des MTGs wird aus einem CAD-Modell eine Vuforia Datenbank generiert, welche als \emph{.unitypackage} heruntergeladen und in Unity importiert werden kann. Es ist möglich, die Datenbank mit Cloudbasiertem Deep-Learning zu trainieren und so eine \emph{Advanced Model Target} Datenbank zu erhalten, dies wurde allerdings in dieser Arbeit nicht getan. 

\textbf{Guide Views}

Bei einer untrainierten Datenbank muss das zu trackende Objekt von einer bestimmten Position  und einem bestimmten Winkel aus mit der Hololens angeschaut werden, damit das Tracking beginnen kann. Dazu lassen sich im MTG ein oder mehr Bilder generieren, die während der Anwendung dem Nutzer angezeigt werden. Sie überlagern das Gesehene mit Kanten des 3D-Modells (siehe Abb.\ref{fig:4.1}). Der Nutzer muss sich dann so ausrichten, dass Objekt und Hilfsbild übereinstimmen. So hilft das Bild dem Nutzer so dabei, sich richtig zu positionieren und die Hololens im korrekten Winkel zum zu trackenden Objekt auszurichten. Als \emph{Guide View} wird in Vuforia sowohl das Hilfsbild, als auch Winkel und Position, die relativ zum zu trackenden Objekt eingenommen werden müssen bezeichnet. 

\begin{figure}[ht]
	\centering
		\includegraphics[width=0.75\textwidth]{images/guideView.jpg}
	\caption[Vuforia Guide View]{Guide View auf einem Android Gerät (https://library.vuforia.com/features/objects/model-targets.html) 13.08.2019}
	\label{fig:4.1}
\end{figure}

\subsection{Vuforia Image Targets}

%https://library.vuforia.com/articles/Training/Image-Target-Guide 14.08.2019
%https://library.vuforia.com/content/vuforia-library/en/articles/Training/Extended-Tracking.html
Ein \emph{Vuforia Image Target} ist eine Art Fiducial, das von Vuforia getrackt werden kann. Dabei muss zuerst eine Datenbank im Target Manager des Vuforia Developer Portals angelegt werden. Anschließend kann diese als \emph{.unitypackage} heruntergeladen und in das Unityprojekt importiert werden. Die Vuforia Engine findet und trackt die natürlichen Merkmale in einem Target indem sie diese mit der Datenbank abgleicht. 

In Unity kann einem Image Target ein GameObject als Kind hinzugefügt werden. Wird dieses Image Target erkannt und getrackt, wird das Kind des Image Targets gerendert. Vuforia trackt ein Target solange es teilweise im Sichtfeld der Kamera ist. Ist das Target nicht mehr im Sichtfeld der Kamera und kann somit nicht mehr getrackt werden, wird auch das Kind nicht mehr gerendert. Mit Hilfe von \emph{Extended Tracking} kann die Robustheit des Trackings erhöht werden. Extended Tracking bedeutet, dass die Pose des Targets bekannt bleibt, obwohl das Target sich nicht mehr im Sichtfeld des Gerätes befindet. Dies wird mit Hilfe des \emph{Positional Device Trackers} erreicht, der seit der Vuforia Engine 7.2 standardmäßig aktiviert ist. Durch Extended Tracking ist es möglich, komplexe Hologramme anzuzeigen, bei welchen das Target durch die Hologrammgröße bei Betrachtung nicht konstant im Sichtfeld ist oder bei welchen das Target durch Nutzung von Handgesten stellenweise verdeckt wird.

\textbf{Image Target Optimierung}

Das Vuforia Developer Portal bietet eine Bewertung von 0 bis 5 Sternen für hochgeladene Bilder an. Diese Bewertung sagt aus, wie gut sich das Bild als Image Target eignet, also wie gut das Bild mit Hilfe von Vuforia erkannt und getrackt werden kann. Je höher die Bewertung, desto besser eignet es sich. Ein Bild, welches als Image Target verwendet werden soll, sollte:
\begin{itemize}
\item detailliert sein,
\item hohen Kontrast aufweisen und
\item keine sich wiederholenden Muster haben.
\end{itemize} 

Vuforia analysiert ein hochgeladenes Bild auf seine Merkmale und speichert Bild und Merkmale in einer Datenbank ab. Unter Merkmal versteht Vuforia "ein scharfes, spitzes, kantiges Detail im Bild". Desweiteren bezeichnet Vuforia die Merkmale als "kontrastbasierte Merkmale". Da Vuforia nicht open source ist, kann keine genauere Aussage über den von Vuforia genutzten Computer Vision Algorithmus zur Merkmalserkennung getroffen werden.
Die gefundenen Merkmale eines Bildes werden im Target Manager des Vuforia Developer Portals als gelbe Kreuze angezeigt. 

\textbf{Genutzte Image Targets}

Die in dieser Arbeit genutzten Marker wurden mit einem AR Marker Generator \footnote{https://shawnlehner.github.io/ARMaker/ (Stand 06.07.2019)} erstellt. Ihnen wurde eine Nummer in der linken oberen Ecke hinzugefügt, um sie unterscheidbar zu machen. Sie wurden auf nicht-glänzendem Papier in der Größe 5cm x 5cm gedruckt. Damit sie nicht flexibel sind, wurden sie auf einem Stück Pappe befestigt. Im Target Manager des Vuforia Developer Portals haben sie eine 5-Sterne-Bewertung erhalten, eignen sich also gut als Image Targets.


\begin{figure}[ht]
	\centering
		\includegraphics[width=1\textwidth]{images/features.png}
	\caption[Features]{Links: Merkmale im Beispielbild von Vuforia, Rechts: Merkmale der in dieser Arbeit verwendeten Marker}
	\label{fig:4.2}
\end{figure}

\section{Versuchsaufbau}

Ziel dieser Arbeit ist, ein Hologramm mit Hilfe der Hololens ohne externes Tracking System an einem Phantom zu Registrieren. Dazu muss das Phantom während der Laufzeit von der Hololens getrackt werden können. Um dies zu erreichen, wurden mehrere Möglichkeiten diskutiert. Einige dieser Möglichkeiten waren Vuforia Cylinder Targets, Point Fiducials und Aruco Marker. 

Vuforia Cylinder Targets wurden ebenfalls von Frantz et al. für ihre Registrierung eines Schädels genutzt. %TODO cite
Dabei handelt es sich um ein Bild, welches außen an einem Zylinder angebracht wird und getrackt werden kann auch wenn der Nutzer sich um den Zylinder herum bewegt. Diese Option wurde jedoch verworfen, da das in dieser Arbeit genutzte Phantom mit einem Torso wesentlich gößer als ein Schädel ist und es daher Probleme gegeben hätte, das Target immer im Blickfeld zu halten. Desweiteren hätte das Target nur schwierig stabil neben dem Phantom positioniert werden können.

Eine weitere Überlegung galt kreisförmigen einfarbigen Point Fiducials. In diesem Fall hätte auf den Kamerastream der Hololens zugegriffen werden müssen. In diesem hätten erst alle Pixel in der Farbe der Fiducials gefunden werden müssen und anschließend beispielsweise mit Hilfe der Hough-Transformation die Fiducials segmentiert werden müssen. Ein Vorteil bei kreisförmigen Fiducials ist, dass die Kreisform relativ invariant zu Verzerrungen ist. Allerdings käme es durch Unterschiede im Lichteinfall zu Farbvarianzen im Kamerastream, sodass es Probleme beim finden der korrekten Farbpixel gegeben hätte. Zusätzlich bietet ein kreisförmiger Fiducial keine 6DoF, sodass zu jeder Zeit mehrere Fiducials sichtbar sein müssten, um die Rotation der Kamera korrekt bestimmen zu können. Aus diesem Grund wurde auch diese Möglichkeit verworfen.

%TODO cite Aruco Marker
Bei der Nutzung von Aruco Markern bietet ein einzelner Marker durch seine planare quadratische Form 6DoF. Da mit Vuforia jedoch schon ein SDK existiert, welches ebenso Marker mit 6DoF bietet und schon in Unity integriert ist, wurden auch die Aruco Marker verworfen.
Implementiert wurde Vuforia Model Target Tracking und Vuforia Image Target Tracking, worauf im Folgenden genauer einegegangen wird. 

\subsection{Model Target Tracking am Phantom}

Es wurde ein Vuforia Model Target Tracking implementiert. Getrackt wurde die Bauchplatte des Phantoms, da nur diese vollständig unbedeckt sichtbar ist. Das CAD-Modell der Bauchplatte wurde in den Model Target Generator geladen. 
%TODO genaueres über Model Target Generator (brauche Internet)
%Wie lange dauert erstellen, wie wurde erstellt guide views etc.
%wie sieht es aus wenn app läuft
%erkennen von model target und wechseln zwischen guide views

Ein Vorteile dieses Ansatzes ist, dass keine zusätzlichen Marker an den Patienten angebracht werden müssen, die etwas verdecken könnten.
Allerdings ist dieser Ansatz dennoch nicht geeignet für eine Anwendung im medizinischen Bereich, da ein 3D-Modell eines realen Patienten nicht die Anforderungen erfüllt, die an ein Model Target gestellt werden. 
%TODO
Zusätzlich ist der zeitige Aufwand, den das Erstellen eines Model Targets mit dem Model Target Generator mit sich bringt sehr viel höher als das Nutzen von planaren Markern, welche an den Patienten angebracht werden. Aus diesem Grund wurde im weiteren Verlauf auf Model Targets verzichtet.

\subsection{Image Target Tracking}

Die Registrierung von Hologramm und Objekt in der realen Welt wurde mit Hilfe von Vuforia Image Targets durchgeführt. Da das Phantom nicht durchgehend verfügbar war, wurde eine Vive Pro Box als zusätzliches Objekt genutzt, an welchem ein in Unity aus einem Würfel erstelltes 3D-Modell registriert wurde. Der Großteil der Entwicklung der Anwendung wurde an dieser Box durchgeführt. %TODO Bilder von Phantom und Box mit Markern
An Box und Phantom wurden Image Targets angebracht. Sie wurden so befestigt, dass sie mit der Hololens von möglichst vielen Positionen aus gut sichtbar sind. 
Beim Phantom wurden die Marker nicht auf der Bauchplatte angebracht, da diese abnehmbar ist und somit eine vorher durchgeführte Kalibrierung durch eine verschoben abgelegte Bauchplatte ungenau werden würde.

In Unity hängen die 3D-Modelle von Box und Phantom jeweils an einem leeren Gameobjekt. Dieses Gameobjekt besitzt ein Script \emph{ModelPositionUpdater.cs} welches die Hologramme während der Laufzeit positioniert. Jedes Image Target hat ein \emph{TargetData.cs} Script, in welchem Positions- und Rotationsoffset vom Target zum Ursprung des entsprechenden Modells (Box oder Phantom) nach Kalibrierung festgehalten wird. Jedes ImageTarget kennt also die Pose, an der das 3D-Modell in Relation zu sich selbst liegt. Wird ein Target während der Laufzeit erkannt und getrackt, nutzt der ModelPositionUpdater diesen Offset zu Positionierung des Hologramms. Aufgrund der 6DoF der Target wäre ein einzelnes Target theoretisch ausreichend, um das Hologramm korrekt zu positionieren. Da sich der Träger der Hololens jedoch um das Phantom bzw. die Box herum bewegen kann, ist ein Target nicht durchgehend sichtbar. Es kommt zusätzlich vor, dass aufgrund eines zu spitzen Winkels zwischen Target und Hololenskamera die Rotation des Targets von Vuforia nicht korrekt erkannt wird. Aus diesen Gründen wurden mehrere Targets angebracht. Der ModelPositionUpdater bildet aus den Offsetvektoren aller zurzeit getrackten und kalibrierten Targets, sowie auch aus den Offsetquaternionen dieser Targets das arithmetische Mittel. Ziel davon ist es, die Positionierung stabiler und genauer gegenüber eventuellen Abweichungen im Tracking oder falscher Kalibrierung eines Markers zu machen.

\begin{figure}[ht]
	\centering
		\includegraphics[width=0.5\textwidth]{images/modelpositionupdater.jpg}
	\caption[Positionierung]{\textbf{(a)} Marker mit lokalem Koordinatensystem; \textbf{(b)} Vektor von Marker zu (c); \textbf{(c)} Position, an welcher das 3D-Modell in Relation zum Marker laut TargetData Script liegt; \textbf{(d)} gemittelte Position und somit Position, an welcher das Hologramm dargestellt wird}
	\label{fig:4.3}
\end{figure}

\section{Kalibrierung}

Damit das 3D-Modell, welches angezeigt werden soll, vom ModelPositionUpdater positioniert werden kann, müssen die Marker kalibriert werden, also Offsetrotation und Offsetposition der Image Targets in das TargetData Script geschrieben werden. Diese Kalibrierung kann auf zwei Wege vorgenommen werden: mit Hilfe der Hololens direkt in der Anwendung oder mit Hilfe eines externen Trackingsystems wie der Polaris. 

\subsection{Kalibrierung mit der Hololens}

Zu Beginn der Kalibrierung mit der Hololens hängt das Hologramm an einem beweglichen Image Target. Mit Hilfe dieses Image Targets kann das Hologramm frei im Raum bewegt werden. Der Träger der Hololens nutzt diesen Marker nun dazu, das Hologramm so gut wie möglich auf dem Objekt (Vive Pro Box oder Phantom) zu positionieren. Der Marker wird auf dem Objekt abgelegt. Sobald einer der fest am Objekt angebrachten Marker im Sichtfeld der Hololens ist, kann die tatsächliche Kalibrierung per Knopfdruck gestartet werden. Das Hologramm hängt nun nicht mehr an dem beweglichen Marker, sondern an den Markern die zum Zeitpunkt des Knopfdrucks sichtbar waren. Nun ist es möglich, das Hologramm über ein Interface entlang X-, Y- oder Z-Achse zu verschieben oder um eine Achse zu rotieren. Ist der Nutzer zufrieden mit der Ausrichtung des Hologramms, wird ein \emph{Save-Button} betätigt, welcher den Offset in Rotation und Position der zu diesem Zeitpunkt von der Kamera sichtbaren Marker in das jeweilige TargetData Script der Marker schreibt. Diese Marker gelten nun als kalibriert und werden von nun an zur Positionierung des Hologramms genutzt. Der Nutzer kann nun von einer anderen Position im Raum schauen, ob die Position des Hologramms korrekt ist. Ist sie dies nicht, kann weiterhin über das Interface eine Feinjustierung der Position und Rotation des Hologramms vorgenommen werden. Anschließend muss der Save-Button betätigt werden, um wiederum für alle zu diesem Zeitpunkt sichtbaren Marker den Offset in das TargetData Script zu schreiben und sie als kalibriert zu markieren. Ist ein bereits als kalibriert markierter Marker zu dem Zeitpunkt sichtbar, so wird der Offset in seinem TargetData Script überschrieben. Dieser Vorgang wird solange wiederholt, bis alle Marker als kalibriert markiert sind. Anschließend kann die Kalibrierung gespeichert und exportiert werden. Bei Neustart der Anwendung kann ebenso eine schon vorher abgeschlossene Kalibrierung geladen werden.

\textbf{Anpassung der Position über das Interface}

Zur Anpassung der Pose (Position und Rotation) über das Interface wurde ein temporärer Offset (\emph{tempOffset}) eingeführt, welcher durch die Buttons des Interfaces geändert werden kann. Der Algorithmus zur Anpassung der Pose wurde in zwei Varianten implementiert. 

\textbf{Variante 1}
%TODO über arbeiten wie in scripten
%TODO 
%TODO dreimal weil wichtig
\begin{itemize}
\item[1] Initiale Positionierung des Hologramms mit Hilfe des beweglichen Markers
\item[2] Sobald mindestens ein anderer Marker sichtbar ist, Start des Kalibrierungsprozesses über Button \emph{Start Calibration}, nutzen der zu diesem Zeitpunkt sichtbar gewesenen Marker zur Positionierung des Hologramms
\item[3] Für als kalibriert markierte Marker gilt \\
 $poseHologram = \mathit{tempOffset} \cdot \mathit{markerOffset} \cdot poseMarker$ \\
 wobei \emph{markerOffset} der bereits kalibrierte Offset im TargetData Script ist
\item[4] Anpassen von \emph{tempOffset} über Interface Buttons
\item[5] Speichern: für alle in diesem Moment sichtbaren Marker gilt \\
$\mathit{markerOffset} = \mathit{tempOffset} \cdot \mathit{markerOffset}$ \\
Anschließend setzen von $\mathit{tempOffset}$ auf $id$ für alle Marker
\end{itemize}

Wie in Abb(noch einzufügen) zu sehen ist, kann es durch diese Methode zu großen Sprüngen des Hologramms kommen, wenn Marker im Sichtfeld sind, von denen einige einen \emph{tempOffset} besitzen, bei anderen jedoch der $\mathit{tempOffset} = id $ ist. Aus diesem Grund wurde Variante 2 implementiert.

\textbf{Variante 2}

\begin{itemize}
\item[1] Initiale Positionierung des Hologramms mit Hilfe des beweglichen Markers
\item[2] Sobald mindestens ein anderer Marker sichtbar ist, Start des Kalibrierungsprozesses über Button \emph{Start Calibration}, nutzen der zu diesem Zeitpunkt sichtbar gewesenen Marker zur Positionierung des Hologramms
\item[3] Für als kalibriert markierte Marker gilt \\
 $poseEstimationHologram = \mathit{markerOffset} \cdot poseMarker$ \\
 $poseHologramFinal = \mathit{tempOffset} \cdot poseEstimationHologram$
\item[4] Speichern: für alle in diesem Moment sichtbaren Marker wird der Offset von Hologramm zu Marker abgespeichert und $\mathit{tempOffset} = id$ gesetzt
\end{itemize} 

\subsection{Kalibrierung mit der Polaris}

\section{GUI und Interaktion}
man muss mit dem Hologramm interagieren wegen kalibrierung.
zwei möglichkeiten für GUI .
1. box transparent.
2. rendering reihenfolge (box muss von achsen überlagert werden).

für 2. entschieden, da bei box transparent buttons drin liegen und so nicht ersichtlich dass interagiert werden kann.


