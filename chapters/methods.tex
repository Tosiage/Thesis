\chapter{\iflanguage{english}{Methods}{Methoden}}
\label{cha:methods}

Im Folgenden Kapitel wird auf die in dieser Arbeit genutzte Hardware und SDKs eingegangen, sowie auf die verschiedenen Ansätze zum Versuchsaufbau, die verfolgt wurden. Es wird des Kalibrierungsvorgang erläutert und zuletzt das User Interface, welches genutzt wurde.

\section{Microsoft Hololens}

Das in dieser Arbeit genutzte Optical See-Through HMD ist die Hololens der ersten Generation. Die Hololens wurde 2016 von der Microsoft Corporation auf den Markt gebracht. Sie übertrifft andere kommerziell verfügbaren OST-HMDs in Kontrast und Stabilität der Bildwiederholrate und hat eine geringere Zeitverzögerung \cite{qian_comparison_2017}. Die Hololens ist nicht kabelgebunden, lässt sich über Handgesten sowie Sprachbefehle steuern und gefährdet somit nicht die Sterilität im Operationssaal \cite{pratt_through_2018}.

Ein virtuelles Objekt, welches von der Hololens angezeigt wird, wird \emph{Hologramm} genannt. Die Hololens projiziert ein Hologramm auf zwei Durchsichtdisplays (\emph{Waveguides)}, für jedes Auge eines. Im Gegensatz zu Video See-Through HMDs, welche bei einem Stromausfall dem Nutzer jegliche Sicht versperren, ermöglichen die Waveguides zeitgleich die Sicht auf die reale Umgebung. Für jeden Waveguide gibt es eine \emph{Light Engine}, welche die holographischen Inhalte projiziert. Mit Hilfe von vier Umgebungssensoren und einer Time-of-Flight Tiefenkamera kann die Hololens eine Karte des Raumes erstellen (\emph{Spatial Mapping}) und sich selbst darin Positionieren. Die Tiefenkamera wird zusätzlich zur Erkennung der Handgesten genutzt. \cite{varnauld_mixed_nodate}
%TODO richtiges Zitat Mixed Reality Documentation

\textbf{Technische Daten}

\begin{center}
\begin{tabular}{ll}
\hline
\textbf{Komponente}    & \textbf{Daten}   \\
\hline
\textbf{Optik}      & 2 Durchsichtdisplays (Waveguides)\\
          & 2 HD 16:9 Light Engines \\
          &\\
\textbf{Holographische Auflösung} & 2,3 Millionen Lichtpunkte\\
  &\\
\textbf{Holographische Dichte} & mehr als 2500 Radianten \\
&(2500 Lichtpunkte pro Radiant, \\
& wobei $1 rad \approx 57,3^\circ $ ) \\
  &\\
\textbf{Sensoren}       & 1 IMU (Inertiale Messeinheit)       \\
& 4 Umgebungssensoren \\
& 1 Tiefenkamera (Time-of-Flight) \\
&1 2MP HD Kamera \\
&4 Mikrofone\\
&1 Umgebungslichtsensor\\
  &\\
\textbf{Interaktion}       & räumlicher Ton           \\
& Blickverfolgung\\
& Handgesten \\
& Sprachsteuerung (bei Internetverbingung)\\
  &\\
\textbf{Leistung} & 2-3 Stunden aktive Nutzung            \\
  &bis zu 2 Wochen standby\\
  &nutzbar während des Ladevorgangs\\
  &passive Kühlung\\
  &\\
\textbf{Prozessoren} &    Intel 32 bit architecture \\
& Microsoft Holographic Processing Unit (HPU 1.0)         \\
  &\\
\textbf{Betriebssystem} & Windows 10           \\
\hline
\end{tabular}
\end{center}



\section{NDI Polaris Spectra}

Die Polaris Spectra von Northern Digital Inc. ist ein Infrarot Tracking System zur Anwendung im medizinischen Bereich. Die Polaris ist als outside-looking-in System konfiguriert und somit nicht sehr mobil. Die zu trackenden Objekte werden mit Markern ausgestattet und von einem stationären Infrarotsensor getrackt. Die Polaris kann sowohl aktive als auch passive Marker tracken. Aktive Marker werden durch ein elektrisches Signal aktiviert und senden Infrarotstrahlung aus. Passive Marker besitzen mindestens vier reflektive Kugeln. Der Infrarotsensor der Polaris sendet Infrarotstrahlung aus, welche von den passiven Markern reflektiert wird. Anhand der erhaltenen Informationen kann der Infrarotsensor die Position und Rotation der Marker bestimmen. 

\textbf{Technische Daten}

%https://www.ndigital.com/medical/products/polaris-family/ 11.08.2019

\begin{center}
\begin{tabular}{ll}
\hline
\textbf{Komponente}    & \textbf{Daten}   \\
\hline
\textbf{Marker}      & passiv, aktiv, kabellos aktiv\\
          &\\
\textbf{Max. Anzahl passiver Marker} & 32 passive Marker\\
  &\\
\textbf{Max. Update Rate} & 60Hz \\
&\\
\textbf{Infrarotsensor Maße} & 613 mm x 104 mm x 86 mm\\
&\\
\textbf{Infrarotsensor Anbringung} &  Anbringung erhöht an Wand oder Stativ\\
&Fixierung an Rückseite\\
&\\
\textbf{Max. Messvolumen} & Beginn Messung in 950 mm Entfernung von Infrarotsensor\\ & 1856 mm x 1470 mm x 2050 mm (Breite x Höhe x Tiefe)\\
\hline
\end{tabular}
\end{center}

\section{Unity}

\emph{Unity} ist eine Gaming Engine entwickelt von Unity Technologies. Sie kann zum Entwickeln von 3D- und 2D-Spielen genutzt werden, sowie unter anderem zum Erstellen von AR- und VR-Anwendungen. Unity unterstützt mehr als 25 Plattformen, darunter auch die Universal Windows Plattform, welche auf der Hololens genutzt wird. Microsoft empfiehlt Unity zum Entwickeln von MR-Anwendungen für die Hololens, mit der Begründung, dass Unity 'die schnellste Methode zum Erstellen einer Mixed Reality-App' sei \cite{varnauld_mixed_nodate}.
%Wie die MR Docs zitieren?
In dieser Arbeit wurde Unity 2018.2.21f1 genutzt.

\section{Mixed Reality Toolkit}

Das \emph{Mixed Reality Toolkit} (MRTK) für Unity ist ein von Microsoft geleitetes Open Source Projekt zur Unterstützung und Beschleunigung der Entwicklung für die Hololens, Windows Mixed Reality immersive (VR) Headsets und OpenVR. Es enthält verschiedene Bauteile in Form von Skripten und vorgefertigten Objekten (Assets). 
Eines dieser Assets ist das \emph{Input Manager Prefab}, das sich unter anderem aus einem \emph{Gaze Manager}, einem \emph{Input Manager} und einem \emph{Focus Manager} Skript zusammensetzt. Mit Hilfe dieser Skripte lässt sich der Blick des Nutzers verfolgen und Gesten erkennen. Die Verfolgung der Blickrichtung ermöglicht das Anzeigen eines Cursors für den Nutzer und dadurch das Anfokussieren einzelner Objekte in der Szene. So besteht die Möglichkeit, Objekte bei Fokussierung farblich hervorzuheben oder weitere Informationen anzuzeigen, die sonst nicht sichtbar sind. 
In dieser Arbeit wurde die MRTK Version 2017.4.3.0 verwendet.
%https://microsoft.github.io/MixedRealityToolkit-Unity/README.html 11.08.2019

\section{Vuforia Augmented Reality SDK}

Vuforia ist ein plattformübergreifendes Software Development Kit zur Entwicklung von Mixed Reality Anwendungen. Vuforia lässt sich sowohl mit iOS- und Android-Geräten als auch mit der Hololens nutzen. Es ist in Unity integriert und ermöglicht durch Methoden der Computer Vision das Tracken von Markern in verschiedenen Formen, wie z.B. das Tracken eines einzelnen Bildes, 3D-Konfigurationen mit mehreren Bildern oder eines 3D Objektes. Vuforia ist nicht open source. Die in dieser Arbeit genutzte Version der Vuforia Engine ist 8.0.10.

Vuforia stellt in der Unity Szene ein eigenes Kamera-Asset zur Verfügung, welches in dieser Arbeit Anstelle des MixedRealityCameraParent-Assets aus dem Mixed Reality Toolkit verwendet wird.

\subsection{Vuforia Model Targets}

%https://library.vuforia.com/features/objects/model-targets.html 12.08.2019
\emph{Vuforia Model Targets} ermöglichen es, Objekte aus der realen Welt basierend auf ihrer Form zu Tracken. Dabei kann es sich um ein kleines Objekt wie ein Spielzeugauto handeln, aber auch um ein großes Objekt, wie ein richtiges Auto. Eine Voraussetzung hierfür ist jedoch, dass ein 3D-Modell (bspw. ein CAD-Modell) des Objektes existiert. Damit das Tracking eines Model Targets gut funktioniert, müssen sowohl das Objekt, als auch das CAD-Modell einige Anforderungen erfüllen.   

Ein Objekt, das getrackt werden soll, sollte:
\begin{itemize}
%Fixed Position in Space
\item Fix im Raum angebracht sein

%Colored or Patterned surface
\item Eine farbige oder gemusterte Oberfläche besitzen

%Sufficient Geometric Detail
\item Komplex sein

%non-flexible and rigid
\item Keine beweglichen Teile haben und nicht verformbar sein
\end{itemize}
%TODO genauer?


Das CAD-Modell sollte:
\begin{itemize}
\item Keine Löcher oder Risse aufweisen
\item Keine fehlenden Teile haben
\item Keine Normalen aufweisen, die in eine andere Richtung als die Oberflächennormale zeigen
\item Keine fehlenden Farbinformationen oder fehlende Textur aufweisen
\item Keine falsche Textur aufweisen
\end{itemize}


\textbf{Vuforia Model Target Generator}

Um ein Model Target für die Verwendung in Unity zu erstellen, hat Vuforia den \emph{Model Target Generator} (MTG) entwickelt. Mit Hilfe des MTGs wird aus einem CAD-Modell eine Vuforia Datenbank generiert, welche als \emph{.unitypackage} heruntergeladen und in Unity importiert werden kann. Es ist möglich, die Datenbank mit Cloudbasiertem Deep-Learning zu trainieren und so eine \emph{Advanced Model Target} Datenbank zu erhalten, dies wurde allerdings in dieser Arbeit nicht getan. 

\textbf{Guide Views}

Bei einer untrainierten Datenbank muss das zu trackende Objekt von einer bestimmten Position  und einem bestimmten Winkel aus mit der Hololens angeschaut werden, damit das Tracking beginnen kann. Dazu lassen sich im MTG ein oder mehr Bilder generieren, die während der Anwendung dem Nutzer angezeigt werden. Sie überlagern das Gesehene mit Kanten des 3D-Modells (siehe Abb.\ref{fig:4.1}). Der Nutzer muss sich dann so ausrichten, dass Objekt und Hilfsbild übereinstimmen. So hilft das Bild dem Nutzer so dabei, sich richtig zu positionieren und die Hololens im korrekten Winkel zum zu trackenden Objekt auszurichten. Als \emph{Guide View} wird in Vuforia sowohl das Hilfsbild, als auch Winkel und Position, die relativ zum zu trackenden Objekt eingenommen werden müssen bezeichnet. 

\begin{figure}[ht]
	\centering
		\includegraphics[width=0.75\textwidth]{images/guideView.jpg}
	\caption[Vuforia Guide View]{Guide View auf einem Android Gerät (https://library.vuforia.com/features/objects/model-targets.html) 13.08.2019}
	\label{fig:4.1}
\end{figure}

\subsection{Vuforia Image Targets}

%https://library.vuforia.com/articles/Training/Image-Target-Guide 14.08.2019
%https://library.vuforia.com/content/vuforia-library/en/articles/Training/Extended-Tracking.html
Ein \emph{Vuforia Image Target} ist eine Art Fiducial, das von Vuforia getrackt werden kann. Dabei muss zuerst eine Datenbank im Target Manager des Vuforia Developer Portals angelegt werden. Anschließend kann diese als \emph{.unitypackage} heruntergeladen und in das Unityprojekt importiert werden. Die Vuforia Engine findet und trackt die natürlichen Merkmale in einem Target indem sie diese mit der Datenbank abgleicht. 

In Unity kann einem Image Target ein GameObject als Kind hinzugefügt werden. Wird dieses Image Target erkannt und getrackt, wird das Kind des Image Targets gerendert. Vuforia trackt ein Target solange es teilweise im Sichtfeld der Kamera ist. Ist das Target nicht mehr im Sichtfeld der Kamera und kann somit nicht mehr getrackt werden, wird auch das Kind nicht mehr gerendert. Mit Hilfe von \emph{Extended Tracking} kann die Robustheit des Trackings erhöht werden. Extended Tracking bedeutet, dass die Pose des Targets bekannt bleibt, obwohl das Target sich nicht mehr im Sichtfeld des Gerätes befindet. Dies wird mit Hilfe des \emph{Positional Device Trackers} erreicht, der seit der Vuforia Engine 7.2 standardmäßig aktiviert ist. Durch Extended Tracking ist es möglich, komplexe Hologramme anzuzeigen, bei welchen das Target durch die Hologrammgröße bei Betrachtung nicht konstant im Sichtfeld ist oder bei welchen das Target durch Nutzung von Handgesten stellenweise verdeckt wird.

\textbf{Image Target Optimierung}

Das Vuforia Developer Portal bietet eine Bewertung von 0 bis 5 Sternen für hochgeladene Bilder an. Diese Bewertung sagt aus, wie gut sich das Bild als Image Target eignet, also wie gut das Bild mit Hilfe von Vuforia erkannt und getrackt werden kann. Je höher die Bewertung, desto besser eignet es sich. Ein Bild, welches als Image Target verwendet werden soll, sollte:
\begin{itemize}
\item detailliert sein,
\item hohen Kontrast aufweisen und
\item keine sich wiederholenden Muster haben.
\end{itemize} 

Vuforia analysiert ein hochgeladenes Bild auf seine Merkmale und speichert Bild und Merkmale in einer Datenbank ab. Unter Merkmal versteht Vuforia "ein scharfes, spitzes, kantiges Detail im Bild". Desweiteren bezeichnet Vuforia die Merkmale als "kontrastbasierte Merkmale". Da Vuforia nicht open source ist, kann keine genauere Aussage über den von Vuforia genutzten Computer Vision Algorithmus zur Merkmalserkennung getroffen werden.
Die gefundenen Merkmale eines Bildes werden im Target Manager des Vuforia Developer Portals als gelbe Kreuze angezeigt. 

\textbf{Genutzte Image Targets}

Die in dieser Arbeit genutzten Marker wurden mit einem AR Marker Generator \footnote{https://shawnlehner.github.io/ARMaker/ (Stand 06.07.2019)} erstellt. Ihnen wurde eine Nummer in der linken oberen Ecke hinzugefügt, um sie unterscheidbar zu machen. Sie wurden auf nicht-glänzendem Papier in der Größe 5cm x 5cm gedruckt. Damit sie nicht flexibel sind, wurden sie auf einem Stück Pappe befestigt. Im Target Manager des Vuforia Developer Portals haben sie eine 5-Sterne-Bewertung erhalten, eignen sich also gut als Image Targets.


\begin{figure}[ht]
	\centering
		\includegraphics[width=0.75\textwidth]{images/features.png}
	\caption[Features]{Links: Merkmale im Beispielbild von Vuforia, Rechts: Merkmale der in dieser Arbeit verwendeten Marker}
	\label{fig:4.2}
\end{figure}

\section{Versuchsaufbau}

\subsection{Modell Target Tracking am Phantom}

Box.

Phantom.

\section{Kalibrierung}
mit Hololens.

mit Polaris.

\section{GUI und Interaktion}
man muss mit dem Hologramm interagieren wegen kalibrierung
zwei möglichkeiten für GUI 
1. box transparent
2. rendering reihenfolge (box muss von achsen überlagert werden)

für 2. entschieden, da bei box transparent buttons drin liegen und so nicht ersichtlich dass interagiert werden kann


