
@misc{noauthor_zotero_nodate,
	title = {Zotero {\textbar} {Email} validation},
	url = {https://www.zotero.org/user/validate/5aa0f908cf292e5369fd},
	urldate = {2019-04-02},
	file = {Zotero | Email validation:C\:\\Users\\anton\\Zotero\\storage\\HLDNTU3Z\\5aa0f908cf292e5369fd.html:text/html}
}

@inproceedings{garon_real-time_2016,
	address = {Merida, Yucatan, Mexico},
	title = {Real-{Time} {High} {Resolution} 3D {Data} on the {HoloLens}},
	isbn = {978-1-5090-3740-7},
	url = {http://ieeexplore.ieee.org/document/7836494/},
	doi = {10.1109/ISMAR-Adjunct.2016.0073},
	abstract = {The recent appearance of augmented reality headsets, such as the Microsoft HoloLens, is a marked move from traditional 2D screen to 3D hologram-like interfaces. Striving to be completely portable, these devices unfortunately suffer multiple limitations, such as the lack of real-time, high quality depth data, which severely restricts their use as research tools. To mitigate this restriction, we provide a simple method to augment a HoloLens headset with much higher resolution depth data. To do so, we calibrate an external depth sensor connected to a computer stick that communicates with the HoloLens headset in real-time. To show how this system could be useful to the research community, we present an implementation of small object detection on HoloLens device.},
	language = {en},
	urldate = {2019-04-02},
	booktitle = {2016 {IEEE} {International} {Symposium} on {Mixed} and {Augmented} {Reality} ({ISMAR}-{Adjunct})},
	publisher = {IEEE},
	author = {Garon, Mathieu and Boulet, Pierre-Olivier and Doironz, Jean-Philippe and Beaulieu, Luc and Lalonde, Jean-Francois},
	month = sep,
	year = {2016},
	pages = {189--191},
	file = {Garon et al. - 2016 - Real-Time High Resolution 3D Data on the HoloLens.pdf:C\:\\Users\\anton\\Zotero\\storage\\NTVLYLNB\\Garon et al. - 2016 - Real-Time High Resolution 3D Data on the HoloLens.pdf:application/pdf}
}

@article{kuhlemann_towards_2017,
	title = {Towards {X}-ray free endovascular interventions – using {HoloLens} for on-line holographic visualisation},
	volume = {4},
	issn = {2053-3713},
	url = {https://digital-library.theiet.org/content/journals/10.1049/htl.2017.0061},
	doi = {10.1049/htl.2017.0061},
	language = {en},
	number = {5},
	urldate = {2019-04-02},
	journal = {Healthcare Technology Letters},
	author = {Kuhlemann, Ivo and Kleemann, Markus and Jauer, Philipp and Schweikard, Achim and Ernst, Floris},
	month = oct,
	year = {2017},
	pages = {184--187},
	file = {Kuhlemann et al. - 2017 - Towards X-ray free endovascular interventions – us.pdf:C\:\\Users\\anton\\Zotero\\storage\\WKDVYMB6\\Kuhlemann et al. - 2017 - Towards X-ray free endovascular interventions – us.pdf:application/pdf}
}

@article{ha_augmented_2016,
	title={Augmented reality in medicine},
  author={Ha, Ho-Gun and Hong, Jaesung},
  journal={Hanyang Medical Reviews},
  volume={36},
  number={4},
  pages={242--247},
  year={2016}
}

@article{birkfellner_head-mounted_2002,
  title={A head-mounted operating binocular for augmented reality visualization in medicine-design and initial evaluation},
  author={Birkfellner, Wolfgang and Figl, Michael and Huber, Klaus and Watzinger, Franz and Wanschitz, Felix and Hummel, Johann and Hanel, Rudolf and Greimel, Wolfgang and Homolka, Peter and Ewers, Rolf and others},
  journal={IEEE Transactions on Medical Imaging},
  volume={21},
  number={8},
  pages={991--997},
  year={2002},
  publisher={IEEE}
}

@inproceedings{koller_real-time_1997,
	address = {Lausanne, Switzerland},
	title = {Real-time vision-based camera tracking for augmented reality applications},
	isbn = {978-0-89791-953-1},
	url = {http://portal.acm.org/citation.cfm?doid=261135.261152},
	doi = {10.1145/261135.261152},
	abstract = {Augmented reality deals with the problem of dynamically augmenting or enhancing (images or live video of) the real world with computer generated data (e.g., graphics of virtual objects). This poses two major problems: (a) determining the precise alignment of real and virtual coordinate frames for overlay, and (b) capturing the 3D environment including camera and object motions. The latter is important for interactive augmented reality applications where users can interact with both real and virtual objects.},
	language = {en},
	urldate = {2019-04-02},
	booktitle = {Proceedings of the {ACM} symposium on {Virtual} reality software and technology  - {VRST} '97},
	publisher = {ACM Press},
	author = {Koller, Dieter and Klinker, Gudrun and Rose, Eric and Breen, David and Whitaker, Ross and Tuceryan, Mihran},
	year = {1997},
	pages = {87--94},
	file = {Koller et al. - 1997 - Real-time vision-based camera tracking for augment.pdf:C\:\\Users\\anton\\Zotero\\storage\\5MIAMVN4\\Koller et al. - 1997 - Real-time vision-based camera tracking for augment.pdf:application/pdf}
}

@article{nicolau_augmented_2011,
	series = {Special {Issue}: {Education} for {Cancer} {Surgeons}},
	title = {Augmented reality in laparoscopic surgical oncology},
	volume = {20},
	issn = {0960-7404},
	url = {http://www.sciencedirect.com/science/article/pii/S0960740411000521},
	doi = {10.1016/j.suronc.2011.07.002},
	abstract = {Minimally invasive surgery represents one of the main evolutions of surgical techniques aimed at providing a greater benefit to the patient. However, minimally invasive surgery increases the operative difficulty since the depth perception is usually dramatically reduced, the field of view is limited and the sense of touch is transmitted by an instrument. However, these drawbacks can currently be reduced by computer technology guiding the surgical gesture. Indeed, from a patient’s medical image (US, CT or MRI), Augmented Reality (AR) can increase the surgeon’s intra-operative vision by providing a virtual transparency of the patient. AR is based on two main processes: the 3D visualization of the anatomical or pathological structures appearing in the medical image, and the registration of this visualization on the real patient. 3D visualization can be performed directly from the medical image without the need for a pre-processing step thanks to volume rendering. But better results are obtained with surface rendering after organ and pathology delineations and 3D modelling. Registration can be performed interactively or automatically. Several interactive systems have been developed and applied to humans, demonstrating the benefit of AR in surgical oncology. It also shows the current limited interactivity due to soft organ movements and interaction between surgeon instruments and organs. If the current automatic AR systems show the feasibility of such system, it is still relying on specific and expensive equipment which is not available in clinical routine. Moreover, they are not robust enough due to the high complexity of developing a real-time registration taking organ deformation and human movement into account. However, the latest results of automatic AR systems are extremely encouraging and show that it will become a standard requirement for future computer-assisted surgical oncology. In this article, we will explain the concept of AR and its principles. Then, we will review the existing interactive and automatic AR systems in digestive surgical oncology, highlighting their benefits and limitations. Finally, we will discuss the future evolutions and the issues that still have to be tackled so that this technology can be seamlessly integrated in the operating room.},
	number = {3},
	urldate = {2019-04-02},
	journal = {Surgical Oncology},
	author = {Nicolau, Stéphane and Soler, Luc and Mutter, Didier and Marescaux, Jacques},
	month = sep,
	year = {2011},
	keywords = {Augmented reality, Computer-assisted surgery, Patient registration, Tracking system},
	pages = {189--201},
	file = {ScienceDirect Snapshot:C\:\\Users\\anton\\Zotero\\storage\\8KJ7UI2L\\S0960740411000521.html:text/html}
}

@inproceedings{fuchs_augmented_1998,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Augmented reality visualization for laparoscopic surgery},
	isbn = {978-3-540-49563-5},
	abstract = {We present the design and a prototype implementation of a three-dimensional visualization system to assist with laparoscopic surgical procedures. The system uses 3D visualization, depth extraction from laparoscopic images, and six degree-of-freedom head and laparoscope tracking to display a merged real and synthetic image in the surgeon’s video-see-through head-mounted display. We also introduce a custom design for this display. A digital light projector, a camera, and a conventional laparoscope create a prototype 3D laparoscope that can extract depth and video imagery.Such a system can restore the physician’s natural point of view and head motion parallax that are used to understand the 3D structure during open surgery. These cues are not available in conventional laparoscopic surgery due to the displacement of the laparoscopic camera from the physician’s viewpoint. The system can also display multiple laparoscopic range imaging data sets to widen the effective field of view of the device. These data sets can be displayed in true 3D and registered to the exterior anatomy of the patient. Much work remains to realize a clinically useful system, notably in the acquisition speed, reconstruction, and registration of the 3D imagery.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} — {MICCAI}’98},
	publisher = {Springer Berlin Heidelberg},
	author = {Fuchs, Henry and Livingston, Mark A. and Raskar, Ramesh and Colucci, D’nardo and Keller, Kurtis and State, Andrei and Crawford, Jessica R. and Rademacher, Paul and Drake, Samuel H. and Meyer, Anthony A.},
	editor = {Wells, William M. and Colchester, Alan and Delp, Scott},
	year = {1998},
	keywords = {Augmented Reality, Conventional Laparoscopic Surgery, Laparoscopic Camera, Motion Parallax, Video Imagery},
	pages = {934--943}
}

@inproceedings{cui_augmented_2017,
	title = {Augmented reality with {Microsoft} {HoloLens} holograms for near infrared fluorescence based image guided surgery},
	volume = {10049},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10049/100490I/Augmented-reality-with-Microsoft-HoloLens-holograms-for-near-infrared-fluorescence/10.1117/12.2251625.short},
	doi = {10.1117/12.2251625},
	abstract = {Near infrared fluorescence (NIRF) based image guided surgery aims to provide vital information to the surgeon in the operating room, such as locations of cancerous tissue that should be resected and healthy tissue that should to be preserved. Targeted molecular markers, such as tumor or nerve specific probes, are used in conjunctions with NIRF imaging and display systems to provide key information to the operator in real-time. One of the major hurdles for the wide adaptation of these imaging systems is the high cost to operate the instruments, large footprint and complexity of operating the systems. The emergence of wearable NIRF systems has addressed these shortcomings by minimizing the imaging and display systems’ footprint and reducing the operational cost. However, one of the major shortcomings for this technology is the replacement of the surgeon’s natural vision with an augmented reality view of the operating room. In this paper, we have addressed this major shortcoming by exploiting hologram technology from Microsoft HoloLens to present NIR information on a color image captured by the surgeon’s natural vision. NIR information is captured with a CMOS sensor with high quantum efficiency in the 800 nm wavelength together with a laser light illumination light source. The NIR image is converted to a hologram that is displayed on Microsoft HoloLens and is correctly co-registered with the operator’s natural eyesight.},
	urldate = {2019-04-02},
	booktitle = {Molecular-{Guided} {Surgery}: {Molecules}, {Devices}, and {Applications} {III}},
	publisher = {International Society for Optics and Photonics},
	author = {Cui, Nan and Kharel, Pradosh and Gruev, Viktor},
	month = feb,
	year = {2017},
	pages = {100490I},
	file = {Full Text PDF:C\:\\Users\\anton\\Zotero\\storage\\KR97GWXH\\Cui et al. - 2017 - Augmented reality with Microsoft HoloLens hologram.pdf:application/pdf;Snapshot:C\:\\Users\\anton\\Zotero\\storage\\LQS8MLKT\\12.2251625.html:text/html}
}

@misc{milgram_taxonomy_1994,
	  title={A taxonomy of mixed reality visual displays},
  author={Milgram, Paul and Kishino, Fumio},
  journal={IEICE TRANSACTIONS on Information and Systems},
  volume={77},
  number={12},
  pages={1321--1329},
  year={1994},
  publisher={The Institute of Electronics, Information and Communication Engineers}
}

@article{wild_robust_2016,
	title = {Robust augmented reality guidance with fluorescent markers in laparoscopic surgery},
	volume = {11},
	issn = {1861-6429},
	url = {https://doi.org/10.1007/s11548-016-1385-4},
	doi = {10.1007/s11548-016-1385-4},
	abstract = {PurposeLaparoscopic interventions require the precise navigation of medical instruments through the patient’s body, while taking critical structures into account. Although numerous concepts have been proposed for displaying subsurface anatomical detail using augmented reality, clinical translation of these methods has suffered from a lack of robustness as well as from cumbersome integration into the clinical workflow. The purpose of this study was to investigate the feasibility of a new approach to intra-operative registration based on fluorescent markers.MethodsThe proposed approach to augmented reality visualization relies on metabolizable fluorescent markers that are attached to the target organ to guide a 2D/3D intra-operative registration algorithm. In an ex vivo porcine study, marker tracking performance is evaluated in the presence of smoke, blood, and tissue in the field of view of the endoscope.ResultsIn contrast to state-of-the-art needle-shaped fiducial markers, the fluorescent markers can be reliably tracked when occluded by smoke, blood or tissue. This makes the new 2D/3D intra-operative registration approach considerably more robust than state-of-the-art marker-based methods.ConclusionAs the concept can be smoothly integrated into the clinical workflow, its potential for application in clinical laparoscopy is high.},
	language = {en},
	number = {6},
	urldate = {2019-04-02},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Wild, Esther and Teber, Dogu and Schmid, Daniel and Simpfendörfer, Tobias and Müller, Michael and Baranski, Ann-Christin and Kenngott, Hannes and Kopka, Klaus and Maier-Hein, Lena},
	month = jun,
	year = {2016},
	keywords = {Augmented reality, Fluorescence-guided surgery, Indocyanine green, Inside-out tracking, Intra-operative registration, Laparoscopy},
	pages = {899--907},
	file = {Springer Full Text PDF:C\:\\Users\\anton\\Zotero\\storage\\C6NGMTE5\\Wild et al. - 2016 - Robust augmented reality guidance with fluorescent.pdf:application/pdf}
}

@inproceedings{vassallo_hologram_2017,
	title = {Hologram stability evaluation for {Microsoft} {HoloLens}},
	volume = {10136},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10136/1013614/Hologram-stability-evaluation-for-Microsoft-HoloLens/10.1117/12.2255831.short},
	doi = {10.1117/12.2255831},
	abstract = {Augmented reality (AR) has an increasing presence in the world of image-guided interventions which is amplified by the availability of consumer-grade head-mounted display (HMD) technology. The Microsoft$^{\textrm{®}}$ HoloLens$^{\textrm{TM}}$ optical passthrough device is at the forefront of consumer technology, as it is the first un-tethered head mounted computer (HMC). It shows promise of effectiveness in guiding clinical interventions, however its accuracy and stability must still be evaluated for the clinical environment. We have developed an evaluative protocol for the HoloLens$^{\textrm{TM}}$ using an optical measurement device to digitize the perceived pose of the rendered hologram. This evaluates the ability of the HoloLens$^{\textrm{TM}}$ to maintain the hologram in its intended pose. The stability is measured when actions are performed that may cause a shift in the holograms’ pose due to errors in its simultaneous localization and mapping. An emphasis is placed on actions that are more likely to be performed in a clinical setting. This will be used to determine the most applicable use cases for this technology in the future and how to minimize errors when in use. Our results show promise of this device’s potential for intraoperative clinical use. Further analysis must be performed to evaluate other potential sources of hologram disruption.},
	urldate = {2019-04-02},
	booktitle = {Medical {Imaging} 2017: {Image} {Perception}, {Observer} {Performance}, and {Technology} {Assessment}},
	publisher = {International Society for Optics and Photonics},
	author = {Vassallo, Reid and Rankin, Adam and Chen, Elvis C. S. and Peters, Terry M.},
	month = mar,
	year = {2017},
	pages = {1013614},
	file = {Full Text PDF:C\:\\Users\\anton\\Zotero\\storage\\7TDEMGHN\\Vassallo et al. - 2017 - Hologram stability evaluation for Microsoft HoloLe.pdf:application/pdf;Snapshot:C\:\\Users\\anton\\Zotero\\storage\\JEKXWFIG\\12.2255831.html:text/html}
}

@article{qian_technical_2017,
	title = {Technical {Note}: {Towards} {Virtual} {Monitors} for {Image} {Guided} {Interventions} - {Real}-time {Streaming} to {Optical} {See}-{Through} {Head}-{Mounted} {Displays}},
	shorttitle = {Technical {Note}},
	url = {http://arxiv.org/abs/1710.00808},
	abstract = {Purpose: Image guidance is crucial for the success of many interventions. Images are displayed on designated monitors that cannot be positioned optimally due to sterility and spatial constraints. This indirect visualization causes potential occlusion, hinders hand-eye coordination, leads to increased procedure duration and surgeon load. Methods: We propose a virtual monitor system that displays medical images in a mixed reality visualization using optical see-through head-mounted displays. The system streams high-resolution medical images from any modality to the head-mounted display in real-time that are blended with the surgical site. It allows for mixed reality visualization of images in head-, world-, or body-anchored mode and can thus be adapted to specific procedural needs. Results: For typical image sizes, the proposed system exhibits an average end-to-end delay and refresh rate of 214 +- 30 ms and 41:4 +- 32:0 Hz, respectively. Conclusions: The proposed virtual monitor system is capable of real-time mixed reality visualization of medical images. In future, we seek to conduct first pre-clinical studies to quantitatively assess the impact of the system on standard image guided procedures.},
	urldate = {2019-04-02},
	journal = {arXiv:1710.00808 [cs]},
	author = {Qian, Long and Unberath, Mathias and Yu, Kevin and Fuerst, Bernhard and Johnson, Alex and Navab, Nassir and Osgood, Greg},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.00808},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Other Computer Science},
	file = {arXiv\:1710.00808 PDF:C\:\\Users\\anton\\Zotero\\storage\\NU9QF99D\\Qian et al. - 2017 - Technical Note Towards Virtual Monitors for Image.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\anton\\Zotero\\storage\\4RZFVXG5\\1710.html:text/html}
}

@inproceedings{itamiya_holographic_nodate,
	title = {The {Holographic} {Human} for {Surgical} {Navigation} using {Microsoft} {HoloLens}},
	url = {https://easychair.org/publications/paper/RkPz},
	doi = {10.29007/wjjx},
	language = {en},
	urldate = {2019-04-02},
	author = {Itamiya, Tomoki and Iwai, Toshinori and Kaneko, Tsuyoshi},
	pages = {26--20},
	file = {Itamiya et al. - The Holographic Human for Surgical Navigation usin.pdf:C\:\\Users\\anton\\Zotero\\storage\\M3Y2PQWQ\\Itamiya et al. - The Holographic Human for Surgical Navigation usin.pdf:application/pdf}
}

@article{mischkowski_application_2006,
	title = {Application of an augmented reality tool for maxillary positioning in orthognathic surgery – {A} feasibility study},
	volume = {34},
	issn = {1010-5182},
	url = {http://www.sciencedirect.com/science/article/pii/S101051820600970X},
	doi = {10.1016/j.jcms.2006.07.862},
	abstract = {Summary
Background
An augmented reality tool for computer assisted surgery named X-Scope® allows visual tracking of real anatomical structures in superposition with volume rendered CT or MRI scans and thus can be used for navigated translocation of bony segments.
Methods
In a feasibility study X-Scope® was used in orthognathic surgery to control the translocation of the maxilla after Le Fort I osteotomy within a bimaxillary procedure. The situation achieved was compared with the pre-operative situation by means of cephalometric analysis on lateral and frontal cephalograms.
Results
The technique was successfully utilized in 5 patients. Maxillary positioning using X-Scope® was accomplished accurately within a range of 1mm. The tool was used in all cases in addition to the usual intra-operative splints. A stand-alone application without conventional control does not yet seem resonable.
Conclusion
Augmented reality tools like X-Scope® may be helpful for controlling maxillary translocation in orthognathic surgery. The application to other interventions in cranio-maxillofacial surgery such as Le Fort III osteotomy, fronto-orbital advancement, and cranial vault reshaping or repair may also be considered.},
	number = {8},
	urldate = {2019-04-03},
	journal = {Journal of Cranio-Maxillofacial Surgery},
	author = {Mischkowski, Robert A. and Zinser, Max J. and Kübler, Alexander C. and Krug, Barbara and Seifert, Ulrich and Zöller, Joachim E.},
	month = dec,
	year = {2006},
	keywords = {augmented reality, bimaxillary osteotomy, computer assisted surgery, cranio-maxillofacial surgery, Le Fort I osteotomy, orthognathic surgery, X-Scope},
	pages = {478--483},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\anton\\Zotero\\storage\\YC9CQ4RZ\\Mischkowski et al. - 2006 - Application of an augmented reality tool for maxil.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\anton\\Zotero\\storage\\5AYNWYGF\\S101051820600970X.html:text/html}
}

@inproceedings{rae_neurosurgical_2018,
	  title={Neurosurgical burr hole placement using the Microsoft HoloLens},
  author={Rae, Emily and Lasso, Andras and Holden, Matthew S and Morin, Evelyn and Levy, Ron and Fichtinger, Gabor},
  booktitle={Medical Imaging 2018: Image-Guided Procedures, Robotic Interventions, and Modeling},
  volume={10576},
  pages={105760T},
  year={2018},
  organization={International Society for Optics and Photonics}
}

@article{pratt_through_2018,
	  title={Through the HoloLens™ looking glass: augmented reality for extremity reconstruction surgery using 3D vascular models with perforating vessels},
  author={Pratt, Philip and Ives, Matthew and Lawton, Graham and Simmons, Jonathan and Radev, Nasko and Spyropoulou, Liana and Amiras, Dimitri},
  journal={European radiology experimental},
  volume={2},
  number={1},
  pages={2},
  year={2018},
  publisher={Springer}
}

@article{shi_[preliminary_2018,
	title = {[{Preliminary} use of {HoloLens} glasses in surgery of liver cancer].},
	volume = {43},
	issn = {1672-7347},
	url = {http://europepmc.org/abstract/med/29886465},
	doi = {10.11817/j.issn.1672-7347.2018.05.007},
	abstract = {Abstract: OBJECTIVE:To establish the preoperative three dimensional (3D) model of liver cancer, and to precisely match the preoperative planning with the...},
	language = {chi},
	number = {5},
	urldate = {2019-04-03},
	journal = {Zhong nan da xue xue bao. Yi xue ban = Journal of Central South University. Medical sciences},
	author = {Shi, L. and Luo, T. and Zhang, L. and Kang, Z. and Chen, J. and Wu, F. and Luo, J.},
	month = may,
	year = {2018},
	pmid = {29886465},
	pages = {500--504},
	file = {Snapshot:C\:\\Users\\anton\\Zotero\\storage\\V8EZX7H4\\29886465.html:text/html}
}

@inproceedings{larrarte_virtual_2016,
	title = {Virtual markers in virtual laparoscopy surgery},
	doi = {10.1109/STSIVA.2016.7743367},
	abstract = {Augmented Reality or AR is a technology characterized by virtual content adding on the perception of reality using the existing environment and overlaying additional information, this digital information is shown in a device display in real time. The aim of this study is to implement some tests for a mobile augmented reality system (MAR) and test the potential of AR development tools in a mobile application for a surgery training system to assist and train laparoscopic or minimal invasive surgery. These tests are the base for the first application to be developed, it will use 3D visualization, depth extraction from medical images and tracking to display information about the video surgery. Augmented reality apps are written in special 3D programs that allow the developer to tie animation or contextual digital information in a computer program to an augmented reality “marker” in the real world. When a computing device AR app or browser plug-in receives digital information from a known marker, it begins to execute the marker's code and layer the correct image or images.},
	booktitle = {2016 {XXI} {Symposium} on {Signal} {Processing}, {Images} and {Artificial} {Vision} ({STSIVA})},
	author = {Larrarte, E. A. and Alban, A. V.},
	month = aug,
	year = {2016},
	keywords = {augmented reality, Augmented reality, medical image processing, surgery, 3D programs, 3D visualization, animation, AR development tools, augmented reality apps, augmented reality marker code, browser plug-in, Cameras, computer animation, computer program, contextual digital information, data visualisation, depth extraction, Engines, Games, laparoscopic surgery, MAR, medical images, minimal invasive surgery, Minimally invasive surgery, mobile application, mobile augmented reality system, mobile computing, surgery training system, Three-dimensional displays, video signal processing, video surgery, virtual content},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\anton\\Zotero\\storage\\VGZ32SQU\\7743367.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\anton\\Zotero\\storage\\EFWN2LI5\\Larrarte und Alban - 2016 - Virtual markers in virtual laparoscopy surgery.pdf:application/pdf}
}

@article{frantz_augmenting_2018,
	  title={Augmenting Microsoft's HoloLens with vuforia tracking for neuronavigation},
  author={Frantz, Taylor and Jansen, Bart and Duerinck, Johnny and Vandemeulebroucke, Jef},
  journal={Healthcare technology letters},
  volume={5},
  number={5},
  pages={221--225},
  year={2018},
  publisher={IET}
}

@inproceedings{gasques_rodrigues_exploring_2017,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '17},
	title = {Exploring {Mixed} {Reality} in {Specialized} {Surgical} {Environments}},
	isbn = {978-1-4503-4656-6},
	url = {http://doi.acm.org/10.1145/3027063.3053273},
	doi = {10.1145/3027063.3053273},
	abstract = {Recent technology advances in both Virtual Reality and Augmented Reality are creating an opportunity for a paradigm shift in the design of human-computer interaction systems. Delving into the Reality-Virtuality Continuum, we find Mixed Reality - systems designed to augment the physical world with virtual entities that embody characteristics of real world objects. In the medical field, Mixed Reality systems can overlay real-time and spatially accurate results onto a patient's body without the need for external screens. The complexity of these systems previously required specialized prototypes, but newly available commercial products like the Microsoft HoloLens make the technology more available. Through a combination of literature review, expert analysis, and prototyping we explore the use of Mixed Reality in healthcare. From the experience of prototyping Patiently and HoloSim, two applications for augmenting medical training, we outline considerations for the future design and development of virtual interfaces grounded in reality.},
	urldate = {2019-04-03},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} {Extended} {Abstracts} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Gasques Rodrigues, Danilo and Jain, Ankur and Rick, Steven R. and Shangley, Liu and Suresh, Preetham and Weibel, Nadir},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {augmented reality, surgery, hololens, mixed reality},
	pages = {2591--2598},
	file = {ACM Full Text PDF:C\:\\Users\\anton\\Zotero\\storage\\VMM7J9I6\\Gasques Rodrigues et al. - 2017 - Exploring Mixed Reality in Specialized Surgical En.pdf:application/pdf}
}

@article{lee_augmented_2018,
	title = {Augmented {Reality} to {Localize} {Individual} {Organ} in {Surgical} {Procedure}},
	volume = {24},
	issn = {2093-3681},
	url = {https://doi.org/10.4258/hir.2018.24.4.394},
	doi = {10.4258/hir.2018.24.4.394},
	abstract = {Lee D, et al. Healthc Inform Res. 2018 Oct;24(4):394-401. https://doi.org/10.4258/hir.2018.24.4.394},
	language = {English},
	number = {4},
	urldate = {2019-04-03},
	journal = {Healthcare Informatics Research},
	author = {Lee, Dongheon and Yi, Jin Wook and Hong, Jeeyoung and Chai, Young Jun and Kim, Hee Chan and Kong, Hyoun-Joong},
	month = oct,
	year = {2018},
	pages = {394--401},
	file = {Full Text PDF:C\:\\Users\\anton\\Zotero\\storage\\BED7CUTZ\\Lee et al. - 2018 - Augmented Reality to Localize Individual Organ in .pdf:application/pdf;Snapshot:C\:\\Users\\anton\\Zotero\\storage\\DBRRWQLB\\search.html:text/html}
}

@inproceedings{grandi_spatially_2014,
	title = {Spatially aware mobile interface for 3D visualization and interactive surgery planning},
	doi = {10.1109/SeGAH.2014.7067098},
	abstract = {While medical images are fundamental in the surgery planning procedure, the process of analysis of such images slice-by-slice is still tedious and inefficient. In this work we introduce a system for exploration of the internal anatomy structures directly on the surface of the real body using a mobile display device as a window to the interior of the patient's body. The method is based on volume visualization of standard computed tomography datasets and augmented reality for interactive visualization of the generated volume. It supports our liver surgery planner method in the analysis of the segmented liver and in the color classification of the vessels. We present a set of experiments showing the system's ability to operate on mobile devices. Quantitative performance results are detailed, and applications in teaching anatomy and doctor-patient communication are discussed.},
	booktitle = {2014 {IEEE} 3nd {International} {Conference} on {Serious} {Games} and {Applications} for {Health} ({SeGAH})},
	author = {Grandi, J. G. and Maciel, A. and Debarba, H. G. and Zanchet, D. J.},
	month = may,
	year = {2014},
	keywords = {augmented reality, medical image processing, surgery, Surgery, 3D visualization, Cameras, data visualisation, medical images, mobile computing, Three-dimensional displays, anatomy teaching, Biomedical imaging, computer aided instruction, computerised tomography, doctor-patient communication, graphical user interfaces, image classification, image colour analysis, image segmentation, interactive surgery planning, interactive systems, interactive visualization, internal anatomy structures, liver, Liver, liver segmentation, liver surgery planner method, Mobile communication, mobile devices, mobile display device, patient body, Planning, quantitative performance, real body surface, spatially aware mobile interface, standard computed tomography datasets, teaching, vessel color classification, volume visualization},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\anton\\Zotero\\storage\\H5BJX9L4\\7067098.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\anton\\Zotero\\storage\\2RYUMAFP\\Grandi et al. - 2014 - Spatially aware mobile interface for 3D visualizat.pdf:application/pdf}
}

@inproceedings{turini_microsoft_2018,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Microsoft} {HoloLens} {Mixed} {Reality} {Surgical} {Simulator} for {Patient}-{Specific} {Hip} {Arthroplasty} {Training}},
	isbn = {978-3-319-95282-6},
	abstract = {Surgical simulation can offer novice surgeons an opportunity to practice skills outside the operating theatre in a safe controlled environment. According to literature evidence, nowadays there are very few training simulators available for Hip Arthroplasty (HA).In a previous study we have presented a physical simulator based on a lower torso phantom including a patient-specific hemi-pelvis replica embedded in a soft synthetic foam. This work explores the use of Microsoft HoloLens technology to enrich the physical patient-specific simulation with the implementation of wearable mixed reality functionalities. Our HA multimodal simulator based on mixed reality using the HoloLens is described by illustrating the overall system, and by summarizing the main phases of the design and development.Finally, we present a preliminary qualitative study with seven subjects (5 medical students, and 2 orthopedic surgeons) showing encouraging results that suggest the suitability of the HoloLens for the proposed application. However, further studies need to be conducted to perform a quantitative test of the registration accuracy of the virtual content, and to confirm qualitative results in a larger cohort of subjects.},
	language = {en},
	booktitle = {Augmented {Reality}, {Virtual} {Reality}, and {Computer} {Graphics}},
	publisher = {Springer International Publishing},
	author = {Turini, Giuseppe and Condino, Sara and Parchi, Paolo Domenico and Viglialoro, Rosanna Maria and Piolanti, Nicola and Gesi, Marco and Ferrari, Mauro and Ferrari, Vincenzo},
	editor = {De Paolis, Lucio Tommaso and Bourdot, Patrick},
	year = {2018},
	keywords = {Augmented reality, Hip arthroplasty, Microsoft HoloLens, Surgical simulation},
	pages = {201--210},
	file = {Springer Full Text PDF:C\:\\Users\\anton\\Zotero\\storage\\7IB6LLQJ\\Turini et al. - 2018 - A Microsoft HoloLens Mixed Reality Surgical Simula.pdf:application/pdf}
}

@article{and_augmented_1998,
	title = {Augmented reality systems for medical applications},
	volume = {17},
	issn = {0739-5175},
	doi = {10.1109/51.677169},
	abstract = {Augmented reality (AR) is a technology in which a computer-generated image is superimposed onto the user's vision of the real world, giving the user additional information generated from the computer model. This technology is different from virtual reality, in which the user is immersed in a virtual world generated by the computer. Rather, the AR system brings the computer into the "world" of the user by augmenting the real environment with virtual objects. Using an AR system, the user's view of the real world is enhanced. This enhancement may be in the form of labels, 3D rendered models, or shaded modifications. In this article, the authors review some of the research involving AR systems, basic system configurations, image-registration approaches, and technical problems involved with AR technology. They also touch upon the requirements for an interventive AR system, which can help guide surgeons in executing a surgical plan.},
	number = {3},
	journal = {IEEE Engineering in Medicine and Biology Magazine},
	author = {{and} and and, and},
	month = may,
	year = {1998},
	keywords = {Augmented reality, biomedical equipment, Calibration, Computer Graphics, computer-generated image, medical image processing, Sensitivity and Specificity, surgery, Surgery, User-Computer Interface, Video Recording, Engines, image registration, Biomedical imaging, 3D rendered models, augmented reality systems, basic system configurations, Biomedical equipment, Buildings, computer model, Computer vision, Data Display, Humans, Image Processing, Computer-Assisted, image-registration approaches, labels, Medical services, Psychomotor Performance, real world view enhancement, reviews, sensory aids, shaded modifications, Software prototyping, superimposed image, Surges, surgical plan execution guidance, Surgical Procedures, Operative, technical problems, Therapy, Computer-Assisted, Ultrasonography},
	pages = {49--58},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\anton\\Zotero\\storage\\F2LDMUI3\\677169.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\anton\\Zotero\\storage\\Z2NWBIXT\\and und and - 1998 - Augmented reality systems for medical applications.pdf:application/pdf}
}

@article{masutani_augmented_1998,
	title = {Augmented reality visualization system for intravascular neurosurgery},
	volume = {3},
	copyright = {Copyright © 1999 Wiley‐Liss, Inc.},
	issn = {1097-0150},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-0150%281998%293%3A5%3C239%3A%3AAID-IGS3%3E3.0.CO%3B2-B},
	doi = {10.1002/(SICI)1097-0150(1998)3:5<239::AID-IGS3>3.0.CO;2-B},
	abstract = {We aimed to construct an augmented reality-based visualization system to support intravascular neurosurgery and evaluate it in clinical environments. Three-dimensional (3D) vascular models are overlaid on motion pictures from X-ray fluoroscopy by 2D/3D registration using fiducial markers. The models are reconstructed from 3D data obtained from X-ray computed tomographic angiography or from magnetic resonance angiography using the marching-cube algorithm. Intraoperative X-ray images are mapped as texture patterns on a screen object which is displayed with the vascular models. Distortion of X-ray fluoroscopy is eliminated by a new technique of screen mesh deformation. A quantity called reprojection distance was introduced to evaluate the reliability of the displayed images. It predicts the maximum registration error around the registered objects. Analyses of reprojection distances were performed using synthetic data consisting of marker coordinates with 2D or 3D errors. The tolerance of reprojection distance for the clinical environment was determined to be 3.0 mm. The system was tested in two clinical cases in which reprojection distances of 2.6 and 2.09 mm were obtained. Construction and evaluation of our prototype system were successfully carried out. Further development is planned employing a range sensor to permit markerless registration. Comp Aid Surg 3:239–247 (1998). © 1999 Wiley-Liss, Inc.},
	language = {en},
	number = {5},
	urldate = {2019-04-03},
	journal = {Computer Aided Surgery},
	author = {Masutani, Yoshitaka and Dohi, Takeyoshi and Yamane, Fumitaka and Iseki, Hiroshi and Takakura, Kintomo},
	year = {1998},
	keywords = {augmented reality, 2D/3D registration, distortion correction, fiducial markers, intravascular surgery},
	pages = {239--247},
	file = {Snapshot:C\:\\Users\\anton\\Zotero\\storage\\P77SPKDT\\(SICI)1097-0150(1998)35239AID-IGS33.0.html:text/html}
}

@article{shuhaiber_augmented_2004,
	title={Augmented reality in surgery},
  author={Shuhaiber, Jeffrey H},
  journal={Archives of surgery},
  volume={139},
  number={2},
  pages={170--174},
  year={2004},
  publisher={American Medical Association}
}

@article{wagner_virtual_1997,
	title = {Virtual reality for orthognathic surgery: {The} augmented reality environment concept},
	volume = {55},
	issn = {0278-2391},
	shorttitle = {Virtual reality for orthognathic surgery},
	url = {http://www.sciencedirect.com/science/article/pii/S0278239197906893},
	doi = {10.1016/S0278-2391(97)90689-3},
	abstract = {Purpose: The objective of this study was to apply virtual reality technology to osteotomies of the facial skeleton. Materials and Methods: Augmented reality can be considered a hybrid of virtual and real environment spaces, which are coregistered and simultaneously visualized. Using a see-through HMD (head-mounted display) and Interventional Video Tomography intraoperatively, partial visual immersion into a patient-related virtual data space augments the surgeon's perception as shown in an experimental study and clinical applications. Results: Without limiting the surgical judgment, offering continuous observation of the operating field, the presented technology additionally provides visual access to invisible data of anatomy, physiology, and function and thus guarantees unencumbered and fluent surgery. Conclusion: Despite current shortcomings, augmented reality technology proved to be particularly well suited for use in osteotomies of the facial skeleton.},
	number = {5},
	urldate = {2019-04-03},
	journal = {Journal of Oral and Maxillofacial Surgery},
	author = {Wagner, Arne and Rasse, Michael and Millesi, Werner and Ewers, Rolf},
	month = may,
	year = {1997},
	pages = {456--462},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\anton\\Zotero\\storage\\RAYFE6YX\\Wagner et al. - 1997 - Virtual reality for orthognathic surgery The augm.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\anton\\Zotero\\storage\\D6I9MRUL\\S0278239197906893.html:text/html}
}

@misc{noauthor_openhelp_nodate,
	  title={OpenHELP (Heidelberg laparoscopy phantom): development of an open-source surgical evaluation and training tool},
  author={Kenngott, HG and W{\"u}nscher, JJ and Wagner, M and Preukschas, A and Wekerle, AL and Neher, P and Suwelack, S and Speidel, S and Nickel, F and Oladokun, D and others},
  journal={Surgical endoscopy},
  volume={29},
  number={11},
  pages={3338--3347},
  year={2015},
  publisher={Springer}
}

@article{paredes_vuforia_nodate,
	title = {Vuforia v1.5 {SDK}. {Analysis} and evaluation of capabilities},
	language = {en},
	author = {Paredes, Josep and Simonetti, Alexandro},
	pages = {119},
	file = {Paredes und Simonetti - Vuforia v1.5 SDK. Analysis and evaluation of capab.pdf:C\:\\Users\\anton\\Zotero\\storage\\G5K4WRCA\\Paredes und Simonetti - Vuforia v1.5 SDK. Analysis and evaluation of capab.pdf:application/pdf}
}

@inproceedings{and_mobile_2017,
	title = {A mobile augmented reality system for exhibition hall based on {Vuforia}},
	doi = {10.1109/ICIVC.2017.7984714},
	abstract = {Mobile augmented reality (MAR) is a newly-emerging technology which covers the real scene with virtual information by utilizing the mobile terminal and thus enables users to have a better understanding for and interaction with the real environment. The article makes a research on the application of MAR technology in the smart exhibition hall, expounds the technological principles and key technologies of MAR in detail and introduces the Vuforia augmented reality framework. Meanwhile, it also designs and realizes the MAR system of the exhibition hall based on Vuforia, explains the systematic structure, recognition of the creating method of the target library and working process of the system, conducts application experiments on the system and then makes summarizations and analysis on the system effects. Finally, the article discusses the deficiencies on the system and its further work.},
	booktitle = {2017 2nd {International} {Conference} on {Image}, {Vision} and {Computing} ({ICIVC})},
	author = {{and}},
	month = jun,
	year = {2017},
	keywords = {augmented reality, mobile augmented reality system, mobile computing, Databases, exhibition hall, exhibitions, Image recognition, MAR technology, mobile augmented reality, mobile terminal, smart exhibition hall, Target recognition, virtual information, vuforia, Vuforia augmented reality framework},
	pages = {1049--1052},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\anton\\Zotero\\storage\\T5GYGA36\\7984714.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\anton\\Zotero\\storage\\KGURSWE6\\and - 2017 - A mobile augmented reality system for exhibition h.pdf:application/pdf}
}

@article{lepetit_monocular_2005,
	title = {Monocular {Model}-{Based} 3D {Tracking} of {Rigid} {Objects}: {A} {Survey}},
	volume = {1},
	issn = {1572-2740, 1572-2759},
	shorttitle = {Monocular {Model}-{Based} 3D {Tracking} of {Rigid} {Objects}},
	url = {https://www.nowpublishers.com/article/Details/CGV-001},
	doi = {10.1561/0600000001},
	abstract = {Monocular Model-Based 3D Tracking of Rigid Objects: A Survey},
	language = {English},
	number = {1},
	urldate = {2019-04-25},
	journal = {Foundations and Trends® in Computer Graphics and Vision},
	author = {Lepetit, Vincent and Fua, Pascal},
	month = aug,
	year = {2005},
	keywords = {Marker Tracking},
	pages = {1--89},
	file = {0600000001.pdf:C\:\\Users\\anton\\Zotero\\storage\\QTMAVR7I\\0600000001.pdf:application/pdf;inhaltsverzeichnis.pdf:C\:\\Users\\anton\\Zotero\\storage\\87TFJ5XU\\inhaltsverzeichnis.pdf:application/pdf;Snapshot:C\:\\Users\\anton\\Zotero\\storage\\P3G53FBX\\CGV-001.html:text/html}
}

@article{billinghurst_survey_2015,
	  title={A survey of augmented reality},
  author={Billinghurst, Mark and Clark, Adrian and Lee, Gun and others},
  journal={Foundations and Trends{\textregistered} in Human--Computer Interaction},
  volume={8},
  number={2-3},
  pages={73--272},
  year={2015},
  publisher={Now Publishers, Inc.}
}

@article{rabbi_survey_2013,
	  title={A survey on augmented reality challenges and tracking},
  author={Rabbi, Ihsan and Ullah, Sehat},
  journal={Acta graphica: znanstveni {\v{c}}asopis za tiskarstvo i grafi{\v{c}}ke komunikacije},
  volume={24},
  number={1-2},
  pages={29--46},
  year={2013},
  publisher={Acta Graphica doo}
}

@article{afif_vision-based_nodate,
	title = {Vision-based {Tracking} {Technology} for {Augmented} {Reality}: {A} {Survey}},
	volume = {1},
	abstract = {This paper reviewed some development of tracking techniques used for Augmented Reality. It exposes importance of tracking for building a well AR system. This also presents general principles and some methods that has been used by researchers for developing their techniques from earlier development until recently.},
	language = {en},
	author = {Afif, Fadhil Noer and Basori, Ahmad Hoirul and Saari, Nadzari},
	keywords = {Marker Tracking},
	pages = {5},
	file = {Afif et al. - Vision-based Tracking Technology for Augmented Rea.pdf:C\:\\Users\\anton\\Zotero\\storage\\TJPR7A63\\Afif et al. - Vision-based Tracking Technology for Augmented Rea.pdf:application/pdf}
}

@inproceedings{neumann_self-tracking_1996,
	address = {Hong Kong},
	title = {A self-tracking augmented reality system},
	isbn = {978-0-89791-825-1},
	url = {http://dl.acm.org/citation.cfm?doid=3304181.3304203},
	doi = {10.1145/3304181.3304203},
	language = {en},
	urldate = {2019-04-28},
	booktitle = {Proceedings of the {ACM} {Symposium} on {Virtual} {Reality} {Software} and {Technology}  - {VRST} '96},
	publisher = {ACM Press},
	author = {Neumann, Ulrich and Cho, Youngkwan},
	year = {1996},
	keywords = {Marker Tracking},
	pages = {109--115},
	file = {Neumann und Cho - 1996 - A self-tracking augmented reality system.pdf:C\:\\Users\\anton\\Zotero\\storage\\3AUXQWBP\\Neumann und Cho - 1996 - A self-tracking augmented reality system.pdf:application/pdf}
}

@article{yilmaz_object_2006,
	title = {Object {Tracking}: {A} {Survey}},
	volume = {38},
	issn = {0360-0300},
	shorttitle = {Object {Tracking}},
	url = {http://doi.acm.org/10.1145/1177352.1177355},
	doi = {10.1145/1177352.1177355},
	abstract = {The goal of this article is to review the state-of-the-art tracking methods, classify them into different categories, and identify new trends. Object tracking, in general, is a challenging problem. Difficulties in tracking objects can arise due to abrupt object motion, changing appearance patterns of both the object and the scene, nonrigid object structures, object-to-object and object-to-scene occlusions, and camera motion. Tracking is usually performed in the context of higher-level applications that require the location and/or shape of the object in every frame. Typically, assumptions are made to constrain the tracking problem in the context of a particular application. In this survey, we categorize the tracking methods on the basis of the object and motion representations used, provide detailed descriptions of representative methods in each category, and examine their pros and cons. Moreover, we discuss the important issues related to tracking including the use of appropriate image features, selection of motion models, and detection of objects.},
	number = {4},
	urldate = {2019-05-03},
	journal = {ACM Comput. Surv.},
	author = {Yilmaz, Alper and Javed, Omar and Shah, Mubarak},
	month = dec,
	year = {2006},
	keywords = {Appearance models, contour evolution, feature selection, object detection, object representation, point tracking, shape tracking},
	file = {ACM Full Text PDF:C\:\\Users\\anton\\Zotero\\storage\\85KZQG2S\\Yilmaz et al. - 2006 - Object Tracking A Survey.pdf:application/pdf}
}

@inproceedings{kato_marker_1999,
	title = {Marker tracking and {HMD} calibration for a video-based augmented reality conferencing system},
	doi = {10.1109/IWAR.1999.803809},
	abstract = {We describe an augmented reality conferencing system which uses the overlay of virtual images on the real world. Remote collaborators are represented on virtual monitors which can be freely positioned about a user in space. Users can collaboratively view and interact with virtual objects using a shared virtual whiteboard. This is possible through precise virtual image registration using fast and accurate computer vision techniques and head mounted display (HMD) calibration. We propose a method for tracking fiducial markers and a calibration method for optical see-through HMD based on the marker tracking.},
	booktitle = {Proceedings 2nd {IEEE} and {ACM} {International} {Workshop} on {Augmented} {Reality} ({IWAR}'99)},
	author = {Kato, H. and Billinghurst, M.},
	month = oct,
	year = {1999},
	keywords = {augmented reality, Augmented reality, calibration, Calibration, Computer displays, video signal processing, image registration, Computer vision, Humans, fiducial markers, 3D CSCW, calibration method, Collaboration, Collaborative work, computer displays, Computer interfaces, computer vision, groupware, head mounted display, head-up displays, HMD calibration, marker tracking, optical see-through HMD, overlay, precise virtual image registration, real world, remote collaborators, shared virtual whiteboard, teleconferencing, video-based augmented reality conferencing system, Virtual environment, virtual images, virtual monitors, Virtual reality},
	pages = {85--94},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\anton\\Zotero\\storage\\5GF36DC7\\803809.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\anton\\Zotero\\storage\\TNAICY8F\\Kato und Billinghurst - 1999 - Marker tracking and HMD calibration for a video-ba.pdf:application/pdf}
}

@article{garrido-jurado_automatic_2014,
	title = {Automatic generation and detection of highly reliable fiducial markers under occlusion},
	volume = {47},
	issn = {0031-3203},
	url = {http://www.sciencedirect.com/science/article/pii/S0031320314000235},
	doi = {10.1016/j.patcog.2014.01.005},
	abstract = {This paper presents a fiducial marker system specially appropriated for camera pose estimation in applications such as augmented reality and robot localization. Three main contributions are presented. First, we propose an algorithm for generating configurable marker dictionaries (in size and number of bits) following a criterion to maximize the inter-marker distance and the number of bit transitions. In the process, we derive the maximum theoretical inter-marker distance that dictionaries of square binary markers can have. Second, a method for automatically detecting the markers and correcting possible errors is proposed. Third, a solution to the occlusion problem in augmented reality applications is shown. To that aim, multiple markers are combined with an occlusion mask calculated by color segmentation. The experiments conducted show that our proposal obtains dictionaries with higher inter-marker distances and lower false negative rates than state-of-the-art systems, and provides an effective solution to the occlusion problem.},
	number = {6},
	urldate = {2019-05-07},
	journal = {Pattern Recognition},
	author = {Garrido-Jurado, S. and Muñoz-Salinas, R. and Madrid-Cuevas, F. J. and Marín-Jiménez, M. J.},
	month = jun,
	year = {2014},
	keywords = {Augmented reality, Computer vision, Fiducial marker},
	pages = {2280--2292},
	file = {ScienceDirect Snapshot:C\:\\Users\\anton\\Zotero\\storage\\X8I8RRB5\\S0031320314000235.html:text/html}
}

@article{romero_ramirez_speeded_2018,
	title = {Speeded {Up} {Detection} of {Squared} {Fiducial} {Markers}},
	volume = {76},
	doi = {10.1016/j.imavis.2018.05.004},
	abstract = {Squared planar markers have become a popular method for pose estimation in applications such as autonomous robots, unmanned vehicles and virtual trainers. The markers allow estimating the position of a monocular camera with minimal cost, high robustness, and speed. One only needs to create markers with a regular printer, place them in the desired environment so as to cover the working area, and then registering their location from a set of images. Nevertheless, marker detection is a time-consuming process, especially as the image dimensions grows. Modern cameras are able to acquire high resolutions images, but fiducial marker systems are not adapted in terms of computing speed. This paper proposes a multi-scale strategy for speeding up marker detection in video sequences by wisely selecting the most appropriate scale for detection, identification and corner estimation. The experiments conducted show that the proposed approach outperforms the state-of-the-art methods without sacrificing accuracy or robustness. Our method is up to 40 times faster than the state-of-the-art method, achieving over 1000 fps in 4 K images without any parallelization.},
	journal = {Image and Vision Computing},
	author = {Romero Ramirez, Francisco and Muñoz-Salinas, Rafael and Medina-Carnicer, Rafael},
	month = jun,
	year = {2018},
	keywords = {Aruco},
	file = {Full Text PDF:C\:\\Users\\anton\\Zotero\\storage\\SWR454SC\\Romero Ramirez et al. - 2018 - Speeded Up Detection of Squared Fiducial Markers.pdf:application/pdf}
}

@article{garrido-jurado_generation_2015,
	title = {Generation of fiducial marker dictionaries using {Mixed} {Integer} {Linear} {Programming}},
	volume = {51},
	doi = {10.1016/j.patcog.2015.09.023},
	abstract = {Square-based fiducial markers are one of the most popular approaches for camera pose estimation due to its fast detection and robustness. In order to maximize their error correction capabilities, it is required to use an inner binary codification with a large inter-marker distance. This paper proposes two Mixed Integer Linear Programming (MILP) approaches to generate configurable square-based fiducial marker dictionaries maximizing their inter-marker distance. The first approach guarantees the optimal solution, however, it can only be applied to relatively small dictionaries and number of bits since the computing times are too long for many situations. The second approach is an alternative formulation to obtain suboptimal dictionaries within restricted time, achieving results that still surpass significantly the current state of the art methods.},
	journal = {Pattern Recognition},
	author = {Garrido-Jurado, Sergio and Muñoz-Salinas, Rafael and Madrid-Cuevas, Francisco and Medina-Carnicer, Rafael},
	month = oct,
	year = {2015},
	keywords = {Aruco},
	file = {Full Text PDF:C\:\\Users\\anton\\Zotero\\storage\\4P92S86Z\\Garrido-Jurado et al. - 2015 - Generation of fiducial marker dictionaries using M.pdf:application/pdf}
}

@misc{noauthor_aruco_nodate,
	title = {{ArUco} {Library} {Documentation}},
	url = {https://docs.google.com/document/d/1QU9KoBtjSM2kF6ITOjQ76xqL7H0TEtXriJX5kwi9Kgc/edit?usp=embed_facebook},
	abstract = {ArUco: An efficient library for detection of planar markers and camera pose estimation News: Check out our latest project UcoSLAM  Markers + Keypoints!               Author: Rafael Muñoz Salinas email:rmsalinas@uco.es License and how to cite	2 Getting Support	2 Introduction	3 Markers	4 Enclosed...},
	language = {en-GB},
	urldate = {2019-05-07},
	journal = {Google Docs},
	file = {Snapshot:C\:\\Users\\anton\\Zotero\\storage\\2KRS8RPS\\edit.html:text/html}
}

@article{romero-ramirez_speeded_2018,
	title = {Speeded up detection of squared fiducial markers},
	volume = {76},
	issn = {0262-8856},
	url = {http://www.sciencedirect.com/science/article/pii/S0262885618300799},
	doi = {10.1016/j.imavis.2018.05.004},
	abstract = {Squared planar markers have become a popular method for pose estimation in applications such as autonomous robots, unmanned vehicles and virtual trainers. The markers allow estimating the position of a monocular camera with minimal cost, high robustness, and speed. One only needs to create markers with a regular printer, place them in the desired environment so as to cover the working area, and then registering their location from a set of images. Nevertheless, marker detection is a time-consuming process, especially as the image dimensions grows. Modern cameras are able to acquire high resolutions images, but fiducial marker systems are not adapted in terms of computing speed. This paper proposes a multi-scale strategy for speeding up marker detection in video sequences by wisely selecting the most appropriate scale for detection, identification and corner estimation. The experiments conducted show that the proposed approach outperforms the state-of-the-art methods without sacrificing accuracy or robustness. Our method is up to 40 times faster than the state-of-the-art method, achieving over 1000 fps in 4 K images without any parallelization.},
	urldate = {2019-05-07},
	journal = {Image and Vision Computing},
	author = {Romero-Ramirez, Francisco J. and Muñoz-Salinas, Rafael and Medina-Carnicer, Rafael},
	month = aug,
	year = {2018},
	keywords = {Fiducial markers, Marker mapping, SLAM},
	pages = {38--47},
	file = {ScienceDirect Snapshot:C\:\\Users\\anton\\Zotero\\storage\\EBW3SIYM\\S0262885618300799.html:text/html}
}

@article{romero_ramirez_speeded_2018-1,
	title = {Speeded {Up} {Detection} of {Squared} {Fiducial} {Markers}},
	volume = {76},
	doi = {10.1016/j.imavis.2018.05.004},
	abstract = {Squared planar markers have become a popular method for pose estimation in applications such as autonomous robots, unmanned vehicles and virtual trainers. The markers allow estimating the position of a monocular camera with minimal cost, high robustness, and speed. One only needs to create markers with a regular printer, place them in the desired environment so as to cover the working area, and then registering their location from a set of images. Nevertheless, marker detection is a time-consuming process, especially as the image dimensions grows. Modern cameras are able to acquire high resolutions images, but fiducial marker systems are not adapted in terms of computing speed. This paper proposes a multi-scale strategy for speeding up marker detection in video sequences by wisely selecting the most appropriate scale for detection, identification and corner estimation. The experiments conducted show that the proposed approach outperforms the state-of-the-art methods without sacrificing accuracy or robustness. Our method is up to 40 times faster than the state-of-the-art method, achieving over 1000 fps in 4 K images without any parallelization.},
	journal = {Image and Vision Computing},
	author = {Romero Ramirez, Francisco and Muñoz-Salinas, Rafael and Medina-Carnicer, Rafael},
	month = jun,
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\anton\\Zotero\\storage\\T2P75CKE\\Romero Ramirez et al. - 2018 - Speeded Up Detection of Squared Fiducial Markers.pdf:application/pdf}
}

@article{romero_ramirez_speeded_2018-2,
	title = {Speeded {Up} {Detection} of {Squared} {Fiducial} {Markers}},
	volume = {76},
	doi = {10.1016/j.imavis.2018.05.004},
	abstract = {Squared planar markers have become a popular method for pose estimation in applications such as autonomous robots, unmanned vehicles and virtual trainers. The markers allow estimating the position of a monocular camera with minimal cost, high robustness, and speed. One only needs to create markers with a regular printer, place them in the desired environment so as to cover the working area, and then registering their location from a set of images. Nevertheless, marker detection is a time-consuming process, especially as the image dimensions grows. Modern cameras are able to acquire high resolutions images, but fiducial marker systems are not adapted in terms of computing speed. This paper proposes a multi-scale strategy for speeding up marker detection in video sequences by wisely selecting the most appropriate scale for detection, identification and corner estimation. The experiments conducted show that the proposed approach outperforms the state-of-the-art methods without sacrificing accuracy or robustness. Our method is up to 40 times faster than the state-of-the-art method, achieving over 1000 fps in 4 K images without any parallelization.},
	journal = {Image and Vision Computing},
	author = {Romero Ramirez, Francisco and Muñoz-Salinas, Rafael and Medina-Carnicer, Rafael},
	month = jun,
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\anton\\Zotero\\storage\\SDFHADH3\\Romero Ramirez et al. - 2018 - Speeded Up Detection of Squared Fiducial Markers.pdf:application/pdf}
}

@misc{noauthor_automatic_nodate,
	title = {Automatic generation and detection of highly reliable fiducial markers under occlusion {\textbar} {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S0031320314000235?token=146ACB718A3B190FD667F6A6B20A788307E019FA9718248676730E40A3AB181E5AD34C0B766D2C4E9AB6707D102A13A0},
	language = {en},
	urldate = {2019-05-07},
	doi = {10.1016/j.patcog.2014.01.005},
	file = {Snapshot:C\:\\Users\\anton\\Zotero\\storage\\4T7QJI4S\\S0031320314000235.html:text/html}
}

@article{garrido-jurado_automatic_2014-1,
	title = {Automatic generation and detection of highly reliable fiducial markers under occlusion},
	volume = {47},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320314000235},
	doi = {10.1016/j.patcog.2014.01.005},
	abstract = {This paper presents a ﬁducial marker system specially appropriated for camera pose estimation in applications such as augmented reality and robot localization. Three main contributions are presented. First, we propose an algorithm for generating conﬁgurable marker dictionaries (in size and number of bits) following a criterion to maximize the inter-marker distance and the number of bit transitions. In the process, we derive the maximum theoretical inter-marker distance that dictionaries of square binary markers can have. Second, a method for automatically detecting the markers and correcting possible errors is proposed. Third, a solution to the occlusion problem in augmented reality applications is shown. To that aim, multiple markers are combined with an occlusion mask calculated by color segmentation. The experiments conducted show that our proposal obtains dictionaries with higher inter-marker distances and lower false negative rates than state-of-the-art systems, and provides an effective solution to the occlusion problem.},
	language = {en},
	number = {6},
	urldate = {2019-05-07},
	journal = {Pattern Recognition},
	author = {Garrido-Jurado, S. and Muñoz-Salinas, R. and Madrid-Cuevas, F.J. and Marín-Jiménez, M.J.},
	month = jun,
	year = {2014},
	keywords = {Aruco},
	pages = {2280--2292},
	file = {Garrido-Jurado et al. - 2014 - Automatic generation and detection of highly relia.pdf:C\:\\Users\\anton\\Zotero\\storage\\SCFW4CH7\\Garrido-Jurado et al. - 2014 - Automatic generation and detection of highly relia.pdf:application/pdf}
}

@article{fotouhi_interactive_2019,
	title = {Interactive {Flying} {Frustums} ({IFFs}): spatially aware surgical data visualization},
	volume = {14},
	issn = {1861-6410, 1861-6429},
	shorttitle = {Interactive {Flying} {Frustums} ({IFFs})},
	url = {http://link.springer.com/10.1007/s11548-019-01943-z},
	doi = {10.1007/s11548-019-01943-z},
	abstract = {Purpose: As the trend toward minimally invasive and percutaneous interventions continues, the importance of appropriate surgical data visualization becomes more evident. Ineffective interventional data display techniques that yield poor ergonomics that hinder hand–eye coordination, and therefore promote frustration which can compromise on-task performance up to adverse outcome. A very common example of ineffective visualization is monitors attached to the base of mobile C-arm X-ray systems.
Methods: We present a spatially and imaging geometry-aware paradigm for visualization of ﬂuoroscopic images using Interactive Flying Frustums (IFFs) in a mixed reality environment. We exploit the fact that the C-arm imaging geometry can be modeled as a pinhole camera giving rise to an 11-degree-of-freedom view frustum on which the X-ray image can be translated while remaining valid. Visualizing IFFs to the surgeon in an augmented reality environment intuitively unites the virtual 2D X-ray image plane and the real 3D patient anatomy. To achieve this visualization, the surgeon and C-arm are tracked relative to the same coordinate frame using image-based localization and mapping, with the augmented reality environment being delivered to the surgeon via a state-of-the-art optical see-through head-mounted display.
Results: The root-mean-squared error of C-arm source tracking after hand–eye calibration was determined as 0.43◦ ± 0.34◦ and 4.6 ± 2.7 mm in rotation and translation, respectively. Finally, we demonstrated the application of spatially aware data visualization for internal ﬁxation of pelvic fractures and percutaneous vertebroplasty.
Conclusion: Our spatially aware approach to transmission image visualization effectively unites patient anatomy with X-ray images by enabling spatial image manipulation that abides image formation. Our proof-of-principle ﬁndings indicate potential applications for surgical tasks that mostly rely on orientational information such as placing the acetabular component in total hip arthroplasty, making us conﬁdent that the proposed augmented reality concept can pave the way for improving surgical performance and visuo-motor coordination in ﬂuoroscopy-guided surgery.},
	language = {en},
	number = {6},
	urldate = {2019-06-19},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Fotouhi, Javad and Unberath, Mathias and Song, Tianyu and Gu, Wenhao and Johnson, Alex and Osgood, Greg and Armand, Mehran and Navab, Nassir},
	month = jun,
	year = {2019},
	pages = {913--922},
	file = {Fotouhi et al. - 2019 - Interactive Flying Frustums (IFFs) spatially awar.pdf:C\:\\Users\\anton\\Zotero\\storage\\2RR422N5\\Fotouhi et al. - 2019 - Interactive Flying Frustums (IFFs) spatially awar.pdf:application/pdf}
}

@article{liebmann_pedicle_2019,
	  title={Pedicle screw navigation using surface digitization on the Microsoft HoloLens},
  author={Liebmann, Florentin and Roner, Simon and von Atzigen, Marco and Scaramuzza, Davide and Sutter, Reto and Snedeker, Jess and Farshad, Mazda and F{\"u}rnstahl, Philipp},
  journal={International journal of computer assisted radiology and surgery},
  volume={14},
  number={7},
  pages={1157--1165},
  year={2019},
  publisher={Springer}
}

@article{andress_--fly_2018,
	title = {On-the-fly augmented reality for orthopedic surgery using a multimodal fiducial},
	volume = {5},
	issn = {2329-4302, 2329-4310},
	url = {https://www.spiedigitallibrary.org/journals/Journal-of-Medical-Imaging/volume-5/issue-2/021209/On-the-fly-augmented-reality-for-orthopedic-surgery-using-a/10.1117/1.JMI.5.2.021209.short},
	doi = {10.1117/1.JMI.5.2.021209},
	abstract = {Fluoroscopic x-ray guidance is a cornerstone for percutaneous orthopedic surgical procedures. However, two-dimensional (2-D) observations of the three-dimensional (3-D) anatomy suffer from the effects of projective simplification. Consequently, many x-ray images from various orientations need to be acquired for the surgeon to accurately assess the spatial relations between the patient’s anatomy and the surgical tools. We present an on-the-fly surgical support system that provides guidance using augmented reality and can be used in quasiunprepared operating rooms. The proposed system builds upon a multimodality marker and simultaneous localization and mapping technique to cocalibrate an optical see-through head mounted display to a C-arm fluoroscopy system. Then, annotations on the 2-D x-ray images can be rendered as virtual objects in 3-D providing surgical guidance. We quantitatively evaluate the components of the proposed system and, finally, design a feasibility study on a semianthropomorphic phantom. The accuracy of our system was comparable to the traditional image-guided technique while substantially reducing the number of acquired x-ray images as well as procedure time. Our promising results encourage further research on the interaction between virtual and real objects that we believe will directly benefit the proposed method. Further, we would like to explore the capabilities of our on-the-fly augmented reality support system in a larger study directed toward common orthopedic interventions.},
	number = {2},
	urldate = {2019-06-19},
	journal = {Journal of Medical Imaging},
	author = {Andress, Sebastian and Johnson, Alex and Unberath, Mathias and Winkler, Alexander F. and Yu, Kevin and Fotouhi, Javad and Weidert, Simon and Osgood, Greg M. and Navab, Nassir},
	month = jan,
	year = {2018},
	pages = {021209},
	file = {Full Text PDF:C\:\\Users\\anton\\Zotero\\storage\\JQZA6A5W\\Andress et al. - 2018 - On-the-fly augmented reality for orthopedic surger.pdf:application/pdf;Snapshot:C\:\\Users\\anton\\Zotero\\storage\\A2ILX84E\\1.JMI.5.2.021209.html:text/html}
}

@inproceedings{salb_inpres_2003,
  title={INPRES (intraoperative presentation of surgical planning and simulation results): augmented reality for craniofacial surgery},
  author={Salb, Tobias and Brief, Jakob and Welzel, Thomas and Giesler, Bjoern and Hassfeld, Steffan and Muehling, Joachim and Dillmann, Ruediger},
  booktitle={Stereoscopic Displays and Virtual Reality Systems X},
  volume={5006},
  pages={453--460},
  year={2003},
  organization={International Society for Optics and Photonics}
}

@article{khor_augmented_2016,
	title = {Augmented and virtual reality in surgery—the digital surgical environment: applications, limitations and legal pitfalls},
	volume = {4},
	issn = {2305-5839},
	shorttitle = {Augmented and virtual reality in surgery—the digital surgical environment},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5220044/},
	doi = {10.21037/atm.2016.12.23},
	abstract = {The continuing enhancement of the surgical environment in the digital age has led to a number of innovations being highlighted as potential disruptive technologies in the surgical workplace. Augmented reality (AR) and virtual reality (VR) are rapidly becoming increasingly available, accessible and importantly affordable, hence their application into healthcare to enhance the medical use of data is certain. Whether it relates to anatomy, intraoperative surgery, or post-operative rehabilitation, applications are already being investigated for their role in the surgeons armamentarium. Here we provide an introduction to the technology and the potential areas of development in the surgical arena.},
	number = {23},
	urldate = {2019-06-24},
	journal = {Annals of Translational Medicine},
	author = {Khor, Wee Sim and Baker, Benjamin and Amin, Kavit and Chan, Adrian and Patel, Ketan and Wong, Jason},
	month = dec,
	year = {2016},
	pmid = {28090510},
	pmcid = {PMC5220044},
	file = {PubMed Central Full Text PDF:C\:\\Users\\anton\\Zotero\\storage\\962IY6VB\\Khor et al. - 2016 - Augmented and virtual reality in surgery—the digit.pdf:application/pdf}
}

@article{azuma_survey_1997,
	 title={A survey of augmented reality},
  author={Azuma, Ronald T},
  journal={Presence: Teleoperators \& Virtual Environments},
  volume={6},
  number={4},
  pages={355--385},
  year={1997},
  publisher={MIT Press}
}

@misc{noauthor_ieice_nodate,
	title = {{IEICE} {Paper} on {MR}},
	url = {http://etclab.mie.utoronto.ca/people/paul_dir/IEICE94/ieice.html},
	urldate = {2019-06-25},
	file = {IEICE Paper on MR:C\:\\Users\\anton\\Zotero\\storage\\S46BB24A\\ieice.html:text/html}
}

@misc{milgram_taxonomy_1994-1,
	  title={A taxonomy of mixed reality visual displays},
  author={Milgram, Paul and Kishino, Fumio},
  journal={IEICE TRANSACTIONS on Information and Systems},
  volume={77},
  number={12},
  pages={1321--1329},
  year={1994},
  publisher={The Institute of Electronics, Information and Communication Engineers}
}

@article{noauthor_windows_nodate,
	title = {Windows {Mixed} {Reality} {Documentation}},
	language = {en},
	pages = {1725}
}

@misc{varnauld_mixed_nodate,
	title = {Mixed {Reality} documentation - {Mixed} {Reality}},
	url = {https://docs.microsoft.com/en-us/windows/mixed-reality/},
	urldate = {2019-06-25},
	author = {Microsoft},
	note = {Accessed: 2019-06-25}
}

@misc{bray_what_nodate,
	author = {Microsoft},
	title = {What is mixed reality? - {Mixed} {Reality}},
	shorttitle = {What is mixed reality?},
	url = {https://docs.microsoft.com/en-us/windows/mixed-reality/mixed-reality},
	note = {Accessed: 2019-06-25}
}

@article{kishino_http://vered.rose.utoronto.ca/people/paul_dir/ieice94/ieice.htm_nodate,
	title = {http://vered.rose.utoronto.ca/people/paul\_dir/{IEICE}94/ieice.htm},
	language = {en},
	author = {Kishino, Fumio},
	pages = {16}
}

@book{kenngott_mobile_2018,
	  title={Mobile, real-time, and point-of-care augmented reality is robust, accurate, and feasible: a prospective pilot study},
  author={Kenngott, Hannes G{\"o}tz and Preukschas, Anas Amin and Wagner, Martin and Nickel, Felix and M{\"u}ller, Michael and Bellemann, Nadine and Stock, Christian and Fangerau, Markus and Radeleff, Boris and Kauczor, Hans-Ulrich and others},
  journal={Surgical endoscopy},
  volume={32},
  number={6},
  pages={2958--2967},
  year={2018},
  publisher={Springer}
}

@article{kenngott_mobile_2018-1,
	title = {Mobile, real-time, and point-of-care augmented reality is robust, accurate, and feasible: a prospective pilot study},
	volume = {32},
	issn = {0930-2794, 1432-2218},
	shorttitle = {Mobile, real-time, and point-of-care augmented reality is robust, accurate, and feasible},
	url = {http://link.springer.com/10.1007/s00464-018-6151-y},
	doi = {10.1007/s00464-018-6151-y},
	abstract = {Background  Augmented reality (AR) systems are currently being explored by a broad spectrum of industries, mainly for improving point-of-care access to data and images. Especially in surgery and especially for timely decisions in emergency cases, a fast and comprehensive access to images at the patient bedside is mandatory. Currently, imaging data are accessed at a distance from the patient both in time and space, i.e., at a specific workstation. Mobile technology and 3-dimensional (3D) visualization of radiological imaging data promise to overcome these restrictions by making bedside AR feasible.
Methods  In this project, AR was realized in a surgical setting by fusing a 3D-representation of structures of interest with live camera images on a tablet computer using marker-based registration. The intent of this study was to focus on a thorough evaluation of AR. Feasibility, robustness, and accuracy were thus evaluated consecutively in a phantom model and a porcine model. Additionally feasibility was evaluated in one male volunteer.
Results  In the phantom model (n = 10), AR visualization was feasible in 84\% of the visualization space with high accuracy (mean reprojection error ± standard deviation (SD): 2.8 ± 2.7 mm; 95th percentile = 6.7 mm). In a porcine model (n = 5), AR visualization was feasible in 79\% with high accuracy (mean reprojection error ± SD: 3.5 ± 3.0 mm; 95th percentile = 9.5 mm). Furthermore, AR was successfully used and proved feasible within a male volunteer.
Conclusions  Mobile, real-time, and point-of-care AR for clinical purposes proved feasible, robust, and accurate in the phantom, animal, and single-trial human model shown in this study. Consequently, AR following similar implementation proved robust and accurate enough to be evaluated in clinical trials assessing accuracy, robustness in clinical reality, as well as integration into the clinical workflow. If these further studies prove successful, AR might revolutionize data access at patient bedside.},
	language = {en},
	number = {6},
	urldate = {2019-07-16},
	journal = {Surgical Endoscopy},
	author = {Kenngott, Hannes Götz and Preukschas, Anas Amin and Wagner, Martin and Nickel, Felix and Müller, Michael and Bellemann, Nadine and Stock, Christian and Fangerau, Markus and Radeleff, Boris and Kauczor, Hans-Ulrich and Meinzer, Hans-Peter and Maier-Hein, Lena and Müller-Stich, Beat Peter},
	month = jun,
	year = {2018},
	pages = {2958--2967},
	file = {Kenngott et al. - 2018 - Mobile, real-time, and point-of-care augmented rea.pdf:C\:\\Users\\anton\\Zotero\\storage\\CQQXES3F\\Kenngott et al. - 2018 - Mobile, real-time, and point-of-care augmented rea.pdf:application/pdf}
}

@article{teber_-vitro_2010,
	title = {In-{Vitro} {Evaluation} of a {Soft}-{Tissue} {Navigation} {System} for {Laparoscopic} {Prostatectomy}},
	volume = {24},
	issn = {0892-7790},
	url = {https://www.liebertpub.com/doi/full/10.1089/end.2009.0289},
	doi = {10.1089/end.2009.0289},
	abstract = {Purpose: We introduce a custom-designed phantom model for the in-vitro evaluation of an augmented reality-based soft-tissue navigation system for ultrasound-guided prostate interventions.Materials and Methods: Transrectal ultrasound segmentation of the prostate, navigation aid placement, initial registration, endoscope tracking, and enhanced visualization steps in the navigation procedure were performed to accommodate the actual prostatic motion. In-vitro laparoscopic manipulations simulating surgical procedures were performed by a physician using human prostate specimens. The target visualization error, defining the accuracy of the tracking, is determined by means of a leave-out test strategy by alternately using four navigation aids for endoscope registration and the remaining two navigation aids for accuracy verification.Results: The introduction of the navigation aids lasted approximately 3 minutes. The navigation aids and especially their barbs were visible because of their ultrasound reflecting nature. For each organ, 1000 endoscope registrations were calculated, in which two randomly chosen navigation aids served the purpose of verifying the pose. We were able to demonstrate that the superimposed image could follow automatically the videoendoscopic real-time view. The mean target visualization errors for the respective trials were determined as 0.81 (±0.12) mm, 0.62 (±0.14) mm, and 0.98 (±0.23) mm.Conclusions: The ultrasound-based inside-out navigation system for laparoscopic prostatectomy overcomes the problem of tissue shift and deformation in an in-vitro model. In case of organ movement, the augmented picture with the detected navigation aids could follow the videoendoscopic image using the navigation aids as landmarks.},
	number = {9},
	urldate = {2019-07-26},
	journal = {Journal of Endourology},
	author = {Teber, Dogu and Simpfendörfer, Tobias and Guven, Selcuk and Baumhauer, Matthias and Gözen, Ali Serdar and Rassweiler, Jens},
	month = aug,
	year = {2010},
	pages = {1487--1491},
	file = {Full Text PDF:C\:\\Users\\anton\\Zotero\\storage\\EC2TXZFP\\Teber et al. - 2010 - In-Vitro Evaluation of a Soft-Tissue Navigation Sy.pdf:application/pdf;Snapshot:C\:\\Users\\anton\\Zotero\\storage\\YVK3HGWA\\end.2009.html:text/html}
}

@article{simpfendorfer_augmented_2011,
	title = {Augmented {Reality} {Visualization} {During} {Laparoscopic} {Radical} {Prostatectomy}},
	volume = {25},
	issn = {0892-7790},
	url = {https://www.liebertpub.com/doi/full/10.1089/end.2010.0724},
	doi = {10.1089/end.2010.0724},
	abstract = {Purpose: We present an augmented reality (AR) navigation system that conveys virtual organ models generated from transrectal ultrasonography (TRUS) onto a real laparoscopic video during radical prostatectomy. By providing this additional information about the actual anatomy, we can support surgeons in their working decisions. This work reports the system's first in-vivo application.Materials and Methods: The system uses custom-developed needles with colored heads that are inserted into the prostate as soon as the organ surface is uncovered. These navigation aids are once segmented in three-dimensional (3D) TRUS data that is acquired right after the placement of the needles and then continuously tracked in the laparoscopic video images by the surgical navigation system. The navigation system traces the navigation aids in real time and computes a registration between TRUS image and laparoscopic video based on the two-dimensional-three dimensional (2D-3D) point correspondences. With this registration, the system correctly superimposes TRUS-based 3D information on an additional AR monitor placed next to the normal laparoscopic screen. Surgical navigation guidance took place until the prostate was removed from the rectal wall. Finally, the navigation aids were removed together with the specimen inside the specimen bag.Results: The initial human in-vivo application of the surgical navigation system was successful. No complications occurred, the prostate was removed together with the navigation aids, and the system supported the surgeons as intended with an AR visualization in real time. In case of tissue deformations, changes in the spatial configuration of the navigation aids are detected, which preserves the system from erroneous navigation visualization.Conclusions: Feasibility of the navigation system was shown in the first in-vivo application. TRUS information could be superimposed via AR in real time. To show the benefit for the patient, results obtained from a larger number of trials are needed.},
	number = {12},
	urldate = {2019-07-26},
	journal = {Journal of Endourology},
	author = {Simpfendörfer, Tobias and Baumhauer, Matthias and Müller, Michael and Gutt, Carsten N. and Meinzer, Hans-Peter and Rassweiler, Jens J. and Guven, Selcuk and Teber, Dogu},
	month = oct,
	year = {2011},
	pages = {1841--1845},
	file = {Full Text PDF:C\:\\Users\\anton\\Zotero\\storage\\WRDMVA8U\\Simpfendörfer et al. - 2011 - Augmented Reality Visualization During Laparoscopi.pdf:application/pdf;Snapshot:C\:\\Users\\anton\\Zotero\\storage\\PQX8GM9G\\end.2010.html:text/html}
}

@article{muller_mobile_2013,
	title = {Mobile augmented reality for computer-assisted percutaneous nephrolithotomy},
	volume = {8},
	issn = {1861-6429},
	url = {https://doi.org/10.1007/s11548-013-0828-4},
	doi = {10.1007/s11548-013-0828-4},
	abstract = {Purpose Percutaneous nephrolithotomy (PCNL) plays an integral role in treatment of renal stones. Creating percutaneous renal access is the most important and challenging step in the procedure. To facilitate this step, we evaluated our novel mobile augmented reality (AR) system for its feasibility of use for PCNL.Methods A tablet computer, such as an iPad®®{\textasciicircum}\{{\textbackslash}circledR \}, is positioned above the patient with its camera pointing toward the field of intervention. The images of the tablet camera are registered with the CT image by means of fiducial markers. Structures of interest can be superimposed semi-transparently on the video images. We present a systematic evaluation by means of a phantom study. An urological trainee and two experts conducted 53 punctures on kidney phantoms.Results The trainee performed best with the proposed AR system in terms of puncturing time (mean: 99 s), whereas the experts performed best with fluoroscopy (mean: 59 s). iPad assistance lowered radiation exposure by a factor of 3 for the inexperienced physician and by a factor of 1.8 for the experts in comparison with fluoroscopy usage. We achieve a mean visualization accuracy of 2.5 mm.Conclusions The proposed tablet computer-based AR system has proven helpful in assisting percutaneous interventions such as PCNL and shows benefits compared to other state-of-the-art assistance systems. A drawback of the system in its current state is the lack of depth information. Despite that, the simple integration into the clinical workflow highlights the potential impact of this approach to such interventions.},
	language = {en},
	number = {4},
	urldate = {2019-07-26},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Müller, Michael and Rassweiler, Marie-Claire and Klein, Jan and Seitel, Alexander and Gondan, Matthias and Baumhauer, Matthias and Teber, Dogu and Rassweiler, Jens J. and Meinzer, Hans-Peter and Maier-Hein, Lena},
	month = jul,
	year = {2013},
	keywords = {Augmented reality, Computer vision, CT, Image-guided surgery, Mobile},
	pages = {663--675},
	file = {Springer Full Text PDF:C\:\\Users\\anton\\Zotero\\storage\\4BGRXSAY\\Müller et al. - 2013 - Mobile augmented reality for computer-assisted per.pdf:application/pdf}
}

@article{szeliski_computer_nodate,
	  title={Computer vision: algorithms and applications},
  author={Szeliski, Richard},
  year={2010},
  publisher={Springer Science \& Business Media}
}

@misc{noauthor_doi:10.1016/j.ijom.2004.03.018_nodate,
	title = {doi:10.1016/j.ijom.2004.03.018 {\textbar} {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S090150270400102X?token=34E589A78C13E8251D31C2D0CC18FA61AD37A0EE4A2E36AD59B0A59B3C583F3F621AB629EDBF9AEEC595788673EBB2C8},
	urldate = {2019-08-04},
	file = {doi\:10.1016/j.ijom.2004.03.018 | Elsevier Enhanced Reader:C\:\\Users\\anton\\Zotero\\storage\\9YZ377PN\\S090150270400102X.html:text/html;doi10.1016j.ijom.2004.03.018  Elsevier Enhanced.pdf:C\:\\Users\\anton\\Zotero\\storage\\TC7GXYKF\\doi10.1016j.ijom.2004.03.018  Elsevier Enhanced.pdf:application/pdf}
}

@misc{noauthor_basic_nodate,
	title = {Basic research and 12 years of clinical experience in computer-assisted navigation technology: a review - {ScienceDirect}},
	url = {https://www.sciencedirect.com/science/article/pii/S090150270400102X},
	urldate = {2019-08-04},
	file = {Basic research and 12 years of clinical experience in computer-assisted navigation technology\: a review - ScienceDirect:C\:\\Users\\anton\\Zotero\\storage\\IJUBUTUK\\S090150270400102X.html:text/html;Basic research and 12 years of clinical experience.pdf:C\:\\Users\\anton\\Zotero\\storage\\SVAVUK22\\Basic research and 12 years of clinical experience.pdf:application/pdf}
}

@article{garrido-jurado_automatic_2014-2,
	title = {Automatic generation and detection of highly reliable fiducial markers under occlusion},
	volume = {47},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320314000235},
	doi = {10.1016/j.patcog.2014.01.005},
	abstract = {This paper presents a ﬁducial marker system specially appropriated for camera pose estimation in applications such as augmented reality and robot localization. Three main contributions are presented. First, we propose an algorithm for generating conﬁgurable marker dictionaries (in size and number of bits) following a criterion to maximize the inter-marker distance and the number of bit transitions. In the process, we derive the maximum theoretical inter-marker distance that dictionaries of square binary markers can have. Second, a method for automatically detecting the markers and correcting possible errors is proposed. Third, a solution to the occlusion problem in augmented reality applications is shown. To that aim, multiple markers are combined with an occlusion mask calculated by color segmentation. The experiments conducted show that our proposal obtains dictionaries with higher inter-marker distances and lower false negative rates than state-of-the-art systems, and provides an effective solution to the occlusion problem.},
	language = {en},
	number = {6},
	urldate = {2019-08-04},
	journal = {Pattern Recognition},
	author = {Garrido-Jurado, S. and Muñoz-Salinas, R. and Madrid-Cuevas, F.J. and Marín-Jiménez, M.J.},
	month = jun,
	year = {2014},
	pages = {2280--2292},
	file = {Garrido-Jurado et al. - 2014 - Automatic generation and detection of highly relia.pdf:C\:\\Users\\anton\\Zotero\\storage\\3E2GFPFT\\Garrido-Jurado et al. - 2014 - Automatic generation and detection of highly relia.pdf:application/pdf}
}

@article{garrido-jurado_automatic_2014-3,
	title = {Automatic generation and detection of highly reliable fiducial markers under occlusion},
	volume = {47},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320314000235},
	doi = {10.1016/j.patcog.2014.01.005},
	abstract = {This paper presents a ﬁducial marker system specially appropriated for camera pose estimation in applications such as augmented reality and robot localization. Three main contributions are presented. First, we propose an algorithm for generating conﬁgurable marker dictionaries (in size and number of bits) following a criterion to maximize the inter-marker distance and the number of bit transitions. In the process, we derive the maximum theoretical inter-marker distance that dictionaries of square binary markers can have. Second, a method for automatically detecting the markers and correcting possible errors is proposed. Third, a solution to the occlusion problem in augmented reality applications is shown. To that aim, multiple markers are combined with an occlusion mask calculated by color segmentation. The experiments conducted show that our proposal obtains dictionaries with higher inter-marker distances and lower false negative rates than state-of-the-art systems, and provides an effective solution to the occlusion problem.},
	language = {en},
	number = {6},
	urldate = {2019-08-04},
	journal = {Pattern Recognition},
	author = {Garrido-Jurado, S. and Muñoz-Salinas, R. and Madrid-Cuevas, F.J. and Marín-Jiménez, M.J.},
	month = jun,
	year = {2014},
	pages = {2280--2292},
	file = {Garrido-Jurado et al. - 2014 - Automatic generation and detection of highly relia.pdf:C\:\\Users\\anton\\Zotero\\storage\\QECFTMI6\\Garrido-Jurado et al. - 2014 - Automatic generation and detection of highly relia.pdf:application/pdf}
}

@article{qian_comparison_2017,
	  title={Comparison of optical see-through head-mounted displays for surgical interventions with object-anchored 2D-display},
  author={Qian, Long and Barthel, Alexander and Johnson, Alex and Osgood, Greg and Kazanzides, Peter and Navab, Nassir and Fuerst, Bernhard},
  journal={International journal of computer assisted radiology and surgery},
  volume={12},
  number={6},
  pages={901--910},
  year={2017},
  publisher={Springer}
}

@article{khamene2003augmented,
  title={An augmented reality system for MRI-guided needle biopsies},
  author={Khamene, Ali and Wacker, Frank and Vogt, Sebastian and Azar, Fred and Wendt, Michael and Sauer, Frank and Lewin, Jonathan},
  journal={Studies in health technology and informatics},
  pages={151--157},
  year={2003},
  publisher={IOS Press; 1999}
}

@article{barsom2016systematic,
  title={Systematic review on the effectiveness of augmented reality applications in medical training},
  author={Barsom, EZ and Graafland, M and Schijven, MP},
  journal={Surgical endoscopy},
  volume={30},
  number={10},
  pages={4174--4183},
  year={2016},
  publisher={Springer}
}

@Article{Hansen2010,
  title={Illustrative visualization of 3D planning models for augmented reality in liver surgery},
  author={Hansen, Christian and Wieferich, Jan and Ritter, Felix and Rieder, Christian and Peitgen, Heinz-Otto},
  journal={International journal of computer assisted radiology and surgery},
  volume={5},
  number={2},
  pages={133--141},
  year={2010},
  publisher={Springer}
}

@inproceedings{devernay2001towards,
  title={Towards endoscopic augmented reality for robotically assisted minimally invasive cardiac surgery},
  author={Devernay, Fr{\'e}d{\'e}ric and Mourgues, Fabien and Coste-Mani{\`e}re, {\`E}ve},
  booktitle={Proceedings International Workshop on Medical Imaging and Augmented Reality},
  pages={16--20},
  year={2001},
  organization={IEEE}
}

